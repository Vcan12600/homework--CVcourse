# ===============================================================================================
# The following shows the last analyze fail log message.
# ===============================================================================================

----------------------------------------------------
- Caught exception:
----------------------------------------------------
For primitive[SparseSoftmaxCrossEntropyWithLogits], the input argument[labels_type] must be a type of {Tensor[Int32], Tensor[Int64]}, but got Tensor[UInt32].

----------------------------------------------------
- C++ Call Stack: (For framework developers)
----------------------------------------------------
mindspore\core\utils\check_convert_utils.cc:932 mindspore::CheckAndConvertUtils::CheckTensorSubClass

----------------------------------------------------
- The Traceback of Net Construct Code:
----------------------------------------------------
# 0 In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:417
        if not self.sense_flag:
# 1 In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:418
            return self._no_sens_impl(*inputs)
                   ^
# 2 In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:437
        if self.return_grad:
# 3 In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:433
        loss = self.network(*inputs)
               ^
# 4 In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:122
        return self._loss_fn(out, label)
               ^
# 5 In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:780
        if self.sparse:
# 6 In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:781
            if self.reduction == 'mean':
            ^
# 7 In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:782
                x = self.sparse_softmax_cross_entropy(logits, labels)
                    ^

# ===============================================================================================
# The following shows the IR when the function graphs evaluation fails to help locate the problem.
# You can search the last ------------------------> to the node which is evaluated failure.
# Refer to https://www.mindspore.cn/search?inputValue=analyze_fail.ir to get more instructions.
# ===============================================================================================

# IR entry: @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_11
# Total subgraphs: 140

# Attrs:
training : 1

# Total params: 21
# Params:
%para1_inputs0 : <null>
%para2_inputs1 : <null>
%para3_conv1.weight : <Ref[Tensor[Float32]], (6, 1, 5, 5), ref_key=:conv1.weight>  :  has_default
%para4_conv2.weight : <Ref[Tensor[Float32]], (16, 6, 5, 5), ref_key=:conv2.weight>  :  has_default
%para5_fc1.weight : <Ref[Tensor[Float32]], (120, 784), ref_key=:fc1.weight>  :  has_default
%para6_fc1.bias : <Ref[Tensor[Float32]], (120), ref_key=:fc1.bias>  :  has_default
%para7_fc2.weight : <Ref[Tensor[Float32]], (84, 120), ref_key=:fc2.weight>  :  has_default
%para8_fc2.bias : <Ref[Tensor[Float32]], (84), ref_key=:fc2.bias>  :  has_default
%para9_fc3.weight : <Ref[Tensor[Float32]], (10, 84), ref_key=:fc3.weight>  :  has_default
%para10_fc3.bias : <Ref[Tensor[Float32]], (10), ref_key=:fc3.bias>  :  has_default
%para11_global_step : <Ref[Tensor[Int32]], (1), ref_key=:global_step>  :  has_default
%para12_moments.conv1.weight : <Ref[Tensor[Float32]], (6, 1, 5, 5), ref_key=:moments.conv1.weight>  :  has_default
%para13_moments.conv2.weight : <Ref[Tensor[Float32]], (16, 6, 5, 5), ref_key=:moments.conv2.weight>  :  has_default
%para14_moments.fc1.weight : <Ref[Tensor[Float32]], (120, 784), ref_key=:moments.fc1.weight>  :  has_default
%para15_moments.fc1.bias : <Ref[Tensor[Float32]], (120), ref_key=:moments.fc1.bias>  :  has_default
%para16_moments.fc2.weight : <Ref[Tensor[Float32]], (84, 120), ref_key=:moments.fc2.weight>  :  has_default
%para17_moments.fc2.bias : <Ref[Tensor[Float32]], (84), ref_key=:moments.fc2.bias>  :  has_default
%para18_moments.fc3.weight : <Ref[Tensor[Float32]], (10, 84), ref_key=:moments.fc3.weight>  :  has_default
%para19_moments.fc3.bias : <Ref[Tensor[Float32]], (10), ref_key=:moments.fc3.bias>  :  has_default
%para20_momentum : <Ref[Tensor[Float32]], (), ref_key=:momentum>  :  has_default
%para21_learning_rate : <Ref[Tensor[Float32]], (), ref_key=:learning_rate>  :  has_default

subgraph attr:
training : 1
subgraph instance: mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_11 : 00000216313D14D0
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:416/    def construct(self, *inputs):/
subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_11(%para1_inputs0, %para2_inputs1, %para3_conv1.weight, %para4_conv2.weight, %para5_fc1.weight, %para6_fc1.bias, %para7_fc2.weight, %para8_fc2.bias, %para9_fc3.weight, %para10_fc3.bias, %para11_global_step, %para12_moments.conv1.weight, %para13_moments.conv2.weight, %para14_moments.fc1.weight, %para15_moments.fc1.bias, %para16_moments.fc2.weight, %para17_moments.fc2.bias, %para18_moments.fc3.weight, %para19_moments.fc3.bias, %para20_momentum, %para21_learning_rate) {

#------------------------> 0
  %1(CNode_26) = call @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_12()
      #scope: (Default)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:417/        if not self.sense_flag:/
  Return(%1)
      : (<null>)
      #scope: (Default)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:417/        if not self.sense_flag:/
}
# Order:
#   1: @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_11:CNode_26{[0]: ValueNode<FuncGraph> mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_12}
#   2: @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_11:CNode_27{[0]: ValueNode<Primitive> Return, [1]: CNode_26}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_12 : 00000216313D3F50
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:416/    def construct(self, *inputs):/
subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_12 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_11]() {
  %1(CNode_28) = $(mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_11):MakeTuple(%para1_inputs0, %para2_inputs1)
      : (<Tensor[Float32], (4, 1, 28, 28)>, <Tensor[UInt32], (4)>) -> (<Tuple[Tensor[Float32],Tensor[UInt32]], TupleShape((4, 1, 28, 28), (4))>)
      #scope: (Default)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:416/    def construct(self, *inputs):/

#------------------------> 1
  %2(CNode_29) = UnpackCall_unpack_call(@_no_sens_impl_30, %1)
      : (<Func, NoShape>, <Tuple[Tensor[Float32],Tensor[UInt32]], TupleShape((4, 1, 28, 28), (4))>) -> (<null>)
      #scope: (Default)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:418/            return self._no_sens_impl(*inputs)/
  Return(%2)
      : (<null>)
      #scope: (Default)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:418/            return self._no_sens_impl(*inputs)/
}
# Order:
#   1: @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_12:CNode_29{[0]: ValueNode<UnpackCall> MetaFuncGraph-unpack_call.31, [1]: ValueNode<FuncGraph> _no_sens_impl_30, [2]: CNode_28}
#   2: @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_12:CNode_32{[0]: ValueNode<Primitive> Return, [1]: CNode_29}


subgraph attr:
core : 1
subgraph instance: UnpackCall_13 : 0000021629DFFEF0
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:418/            return self._no_sens_impl(*inputs)/
subgraph @UnpackCall_13(%para22_, %para23_) {
  %1(CNode_29) = TupleGetItem(%para23_15, I64(0))
      : (<Tuple[Tensor[Float32],Tensor[UInt32]], TupleShape((4, 1, 28, 28), (4))>, <Int64, NoShape>) -> (<Tensor[Float32], (4, 1, 28, 28)>)
      #scope: (Default)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:418/            return self._no_sens_impl(*inputs)/
  %2(CNode_29) = TupleGetItem(%para23_15, I64(1))
      : (<Tuple[Tensor[Float32],Tensor[UInt32]], TupleShape((4, 1, 28, 28), (4))>, <Int64, NoShape>) -> (<Tensor[UInt32], (4)>)
      #scope: (Default)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:418/            return self._no_sens_impl(*inputs)/

#------------------------> 2
  %3(CNode_29) = %para22_14(%1, %2)
      : (<Tensor[Float32], (4, 1, 28, 28)>, <Tensor[UInt32], (4)>) -> (<null>)
      #scope: (Default)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:418/            return self._no_sens_impl(*inputs)/
  Return(%3)
      : (<null>)
      #scope: (Default)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:418/            return self._no_sens_impl(*inputs)/
}
# Order:
#   1: @UnpackCall_13:CNode_29{[0]: param_14, [1]: CNode_29, [2]: CNode_29}
#   2: @UnpackCall_13:CNode_29{[0]: ValueNode<Primitive> Return, [1]: CNode_29}


subgraph attr:
training : 1
subgraph instance: _no_sens_impl_16 : 0000021629DFD470
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:431/    def _no_sens_impl(self, *inputs):/
subgraph @_no_sens_impl_16 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_11](%para24_inputs0, %para25_inputs1) {

#------------------------> 3
  %1(CNode_33) = call @_no_sens_impl_17()
      #scope: (Default)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:437/        if self.return_grad:/
  Return(%1)
      : (<null>)
      #scope: (Default)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:437/        if self.return_grad:/
}
# Order:
#   1: @_no_sens_impl_16:loss{[0]: ValueNode<UnpackCall> MetaFuncGraph-unpack_call.34, [1]: ValueNode<FuncGraph> mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_22, [2]: CNode_35}
#   2: @_no_sens_impl_16:grads{[0]: ValueNode<UnpackGraphPrimitive> UnpackGraph, [1]: ValueNode<FuncGraph> mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_22, [2]: CNode_35}
#   3: @_no_sens_impl_16:grads{[0]: ValueNode<DoSignaturePrimitive> S_Prim_grad, [1]: grads, [2]: CNode_36}
#   4: @_no_sens_impl_16:grads{[0]: ValueNode<UnpackCall> MetaFuncGraph-unpack_call.37, [1]: grads, [2]: CNode_35}
#   5: @_no_sens_impl_16:grads{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Identity_construct_38, [1]: grads}
#   6: @_no_sens_impl_16:CNode_39{[0]: ValueNode<FuncGraph> mindspore_nn_optim_momentum_Momentum_construct_40, [1]: grads}
#   7: @_no_sens_impl_16:loss{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Depend, [1]: loss, [2]: CNode_39}
#   8: @_no_sens_impl_16:CNode_33{[0]: ValueNode<FuncGraph> _no_sens_impl_17}
#   9: @_no_sens_impl_16:CNode_41{[0]: ValueNode<Primitive> Return, [1]: CNode_33}


subgraph attr:
training : 1
subgraph instance: _no_sens_impl_17 : 0000021629E05940
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:431/    def _no_sens_impl(self, *inputs):/
subgraph @_no_sens_impl_17 parent: [subgraph @_no_sens_impl_16]() {

#------------------------> 4
  %1(CNode_42) = call @_no_sens_impl_18()
      #scope: (Default)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:437/        if self.return_grad:/
  Return(%1)
      : (<null>)
      #scope: (Default)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:437/        if self.return_grad:/
}
# Order:
#   1: @_no_sens_impl_17:CNode_42{[0]: ValueNode<FuncGraph> _no_sens_impl_18}
#   2: @_no_sens_impl_17:CNode_43{[0]: ValueNode<Primitive> Return, [1]: CNode_42}


subgraph attr:
training : 1
subgraph instance: _no_sens_impl_18 : 0000021629E063E0
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:431/    def _no_sens_impl(self, *inputs):/
subgraph @_no_sens_impl_18 parent: [subgraph @_no_sens_impl_16]() {
  %1(CNode_35) = $(_no_sens_impl_16):MakeTuple(%para24_inputs0, %para25_inputs1)
      : (<Tensor[Float32], (4, 1, 28, 28)>, <Tensor[UInt32], (4)>) -> (<Tuple[Tensor[Float32],Tensor[UInt32]], TupleShape((4, 1, 28, 28), (4))>)
      #scope: (Default)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:431/    def _no_sens_impl(self, *inputs):/

#------------------------> 5
  %2(loss) = $(_no_sens_impl_16):UnpackCall_unpack_call(@mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_22, %1)
      : (<Func, NoShape>, <Tuple[Tensor[Float32],Tensor[UInt32]], TupleShape((4, 1, 28, 28), (4))>) -> (<null>)
      #scope: (Default)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:433/        loss = self.network(*inputs)/
  %3(grads) = $(_no_sens_impl_16):UnpackGraph(@mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_22, %1)
      : (<null>, <Tuple[Tensor[Float32],Tensor[UInt32]], TupleShape((4, 1, 28, 28), (4))>) -> (<null>)
      #scope: (Default)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:434/        grads = self.grad_no_sens(self.network, self.weights)(*inputs)/
  %4(CNode_36) = $(_no_sens_impl_16):MakeTuple(%para3_conv1.weight, %para4_conv2.weight, %para5_fc1.weight, %para6_fc1.bias, %para7_fc2.weight, %para8_fc2.bias, %para9_fc3.weight, %para10_fc3.bias)
      : (<Ref[Tensor[Float32]], (6, 1, 5, 5)>, <Ref[Tensor[Float32]], (16, 6, 5, 5)>, <Ref[Tensor[Float32]], (120, 784)>, <Ref[Tensor[Float32]], (120)>, <Ref[Tensor[Float32]], (84, 120)>, <Ref[Tensor[Float32]], (84)>, <Ref[Tensor[Float32]], (10, 84)>, <Ref[Tensor[Float32]], (10)>) -> (<null>)
      #scope: (Default)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:434/        grads = self.grad_no_sens(self.network, self.weights)(*inputs)/
  %5(grads) = $(_no_sens_impl_16):S_Prim_grad(%3, %4)
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:434/        grads = self.grad_no_sens(self.network, self.weights)(*inputs)/
  %6(grads) = $(_no_sens_impl_16):UnpackCall_unpack_call(%5, %1)
      : (<null>, <Tuple[Tensor[Float32],Tensor[UInt32]], TupleShape((4, 1, 28, 28), (4))>) -> (<null>)
      #scope: (Default)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:434/        grads = self.grad_no_sens(self.network, self.weights)(*inputs)/
  %7(grads) = $(_no_sens_impl_16):call @mindspore_nn_layer_basic_Identity_construct_38(%6)
      : (<null>) -> (<null>)
      #scope: (Default)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:435/        grads = self.grad_reducer(grads)/
  %8(CNode_39) = $(_no_sens_impl_16):call @mindspore_nn_optim_momentum_Momentum_construct_40(%7)
      : (<null>) -> (<null>)
      #scope: (Default)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:436/        loss = F.depend(loss, self.optimizer(grads))/
  %9(loss) = $(_no_sens_impl_16):S_Prim_Depend[side_effect_propagate: I64(1)](%2, %8)
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:436/        loss = F.depend(loss, self.optimizer(grads))/
  Return(%9)
      : (<null>)
      #scope: (Default)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:442/        return loss/
}
# Order:
#   1: @_no_sens_impl_18:CNode_44{[0]: ValueNode<Primitive> Return, [1]: loss}


subgraph attr:
core : 1
subgraph instance: UnpackCall_19 : 0000021629F82A70
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:433/        loss = self.network(*inputs)/
subgraph @UnpackCall_19(%para26_, %para27_) {
  %1(loss) = TupleGetItem(%para27_21, I64(0))
      : (<Tuple[Tensor[Float32],Tensor[UInt32]], TupleShape((4, 1, 28, 28), (4))>, <Int64, NoShape>) -> (<Tensor[Float32], (4, 1, 28, 28)>)
      #scope: (Default)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:433/        loss = self.network(*inputs)/
  %2(loss) = TupleGetItem(%para27_21, I64(1))
      : (<Tuple[Tensor[Float32],Tensor[UInt32]], TupleShape((4, 1, 28, 28), (4))>, <Int64, NoShape>) -> (<Tensor[UInt32], (4)>)
      #scope: (Default)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:433/        loss = self.network(*inputs)/

#------------------------> 6
  %3(loss) = %para26_20(%1, %2)
      : (<Tensor[Float32], (4, 1, 28, 28)>, <Tensor[UInt32], (4)>) -> (<null>)
      #scope: (Default)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:433/        loss = self.network(*inputs)/
  Return(%3)
      : (<null>)
      #scope: (Default)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:433/        loss = self.network(*inputs)/
}
# Order:
#   1: @UnpackCall_19:loss{[0]: param_20, [1]: loss, [2]: loss}
#   2: @UnpackCall_19:loss{[0]: ValueNode<Primitive> Return, [1]: loss}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_22 : 0000021629E05E90
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:120/    def construct(self, data, label):/
subgraph @mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_22 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_11](%para28_data, %para29_label) {
  %1(out) = call @__main___LeNet5_construct_45(%para28_data)
      : (<Tensor[Float32], (4, 1, 28, 28)>) -> (<Tensor[Float32], (4, 10)>)
      #scope: (Default/network-WithLossCell)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:121/        out = self._backbone(data)/

#------------------------> 7
  %2(CNode_46) = call @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_23(%1, %para29_label)
      : (<Tensor[Float32], (4, 10)>, <Tensor[UInt32], (4)>) -> (<null>)
      #scope: (Default/network-WithLossCell)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:122/        return self._loss_fn(out, label)/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:122/        return self._loss_fn(out, label)/
}
# Order:
#   1: @mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_22:out{[0]: ValueNode<FuncGraph> __main___LeNet5_construct_45, [1]: param_data}
#   2: @mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_22:CNode_46{[0]: ValueNode<FuncGraph> mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_23, [1]: out, [2]: param_label}
#   3: @mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_22:CNode_47{[0]: ValueNode<Primitive> Return, [1]: CNode_46}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_23 : 0000021629F7AAF0
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:777/    def construct(self, logits, labels):/
subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_23(%para30_logits, %para31_labels) {
  %1(CNode_48) = S_Prim__check_is_tensor[constexpr_prim: Bool(1)]("logits", %para30_logits, "SoftmaxCrossEntropyWithLogits")
      : (<String, NoShape>, <Tensor[Float32], (4, 10)>, <String, NoShape>) -> (<None, NoShape>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:778/        _check_is_tensor('logits', logits, self.cls_name)/
  %2(CNode_49) = S_Prim__check_is_tensor[constexpr_prim: Bool(1)]("labels", %para31_labels, "SoftmaxCrossEntropyWithLogits")
      : (<String, NoShape>, <Tensor[UInt32], (4)>, <String, NoShape>) -> (<None, NoShape>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:779/        _check_is_tensor('labels', labels, self.cls_name)/
  %3(CNode_50) = MakeTuple(%1, %2)
      : (<None, NoShape>, <None, NoShape>) -> (<Tuple[None*2], TupleShape(NoShape, NoShape)>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:777/    def construct(self, logits, labels):/
  %4(CNode_51) = StopGradient(%3)
      : (<Tuple[None*2], TupleShape(NoShape, NoShape)>) -> (<Tuple[None*2], TupleShape(NoShape, NoShape)>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:777/    def construct(self, logits, labels):/

#------------------------> 8
  %5(CNode_52) = call @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_24()
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:780/        if self.sparse:/
  %6(CNode_53) = Depend[side_effect_propagate: I64(1)](%5, %4)
      : (<null>, <Tuple[None*2], TupleShape(NoShape, NoShape)>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:780/        if self.sparse:/
  Return(%6)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:780/        if self.sparse:/
}
# Order:
#   1: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_23:CNode_48{[0]: ValueNode<DoSignaturePrimitive> S_Prim__check_is_tensor, [1]: ValueNode<StringImm> logits, [2]: param_logits, [3]: ValueNode<StringImm> SoftmaxCrossEntropyWithLogits}
#   2: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_23:CNode_49{[0]: ValueNode<DoSignaturePrimitive> S_Prim__check_is_tensor, [1]: ValueNode<StringImm> labels, [2]: param_labels, [3]: ValueNode<StringImm> SoftmaxCrossEntropyWithLogits}
#   3: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_23:CNode_52{[0]: ValueNode<FuncGraph> mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_24}
#   4: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_23:CNode_54{[0]: ValueNode<Primitive> Return, [1]: CNode_53}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_24 : 0000021629F7B040
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:777/    def construct(self, logits, labels):/
subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_24 parent: [subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_23]() {
  %1(CNode_55) = S_Prim_equal("mean", "mean")
      : (<String, NoShape>, <String, NoShape>) -> (<Bool, NoShape>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:781/            if self.reduction == 'mean':/
  %2(CNode_56) = Cond(%1, Bool(0))
      : (<Bool, NoShape>, <Bool, NoShape>) -> (<Bool, NoShape>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:781/            if self.reduction == 'mean':/
  %3(CNode_57) = Switch(%2, @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_25, @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_58)
      : (<Bool, NoShape>, <Func, NoShape>, <Func, NoShape>) -> (<Func, NoShape>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:781/            if self.reduction == 'mean':/

#------------------------> 9
  %4(CNode_59) = %3()
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:781/            if self.reduction == 'mean':/
  Return(%4)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:781/            if self.reduction == 'mean':/
}
# Order:
#   1: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_24:CNode_55{[0]: ValueNode<DoSignaturePrimitive> S_Prim_equal, [1]: ValueNode<StringImm> mean, [2]: ValueNode<StringImm> mean}
#   2: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_24:CNode_56{[0]: ValueNode<Primitive> Cond, [1]: CNode_55, [2]: ValueNode<BoolImm> false}
#   3: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_24:CNode_57{[0]: ValueNode<Primitive> Switch, [1]: CNode_56, [2]: ValueNode<FuncGraph> mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_25, [3]: ValueNode<FuncGraph> mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_58}
#   4: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_24:CNode_59{[0]: CNode_57}
#   5: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_24:CNode_60{[0]: ValueNode<Primitive> Return, [1]: CNode_59}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_25 : 0000021629F78070
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:777/    def construct(self, logits, labels):/
subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_25 parent: [subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_23]() {

#------------------------> 10
  %1(x) = S_Prim_SparseSoftmaxCrossEntropyWithLogits[is_grad: Bool(0), sens: F32(1), input_names: ["features", "labels"], output_names: ["output"]](%para30_logits, %para31_labels)
      : (<Tensor[Float32], (4, 10)>, <Tensor[UInt32], (4)>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:782/                x = self.sparse_softmax_cross_entropy(logits, labels)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:783/                return x/
}
# Order:
#   1: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_25:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_SparseSoftmaxCrossEntropyWithLogits, [1]: param_logits, [2]: param_labels}
#   2: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_25:CNode_61{[0]: ValueNode<Primitive> Return, [1]: x}


# ===============================================================================================
# The total of function graphs in evaluation stack: 11/12 (Ignored 1 internal frames).
# ===============================================================================================


# ===============================================================================================
# The rest function graphs are the following:
# ===============================================================================================
subgraph attr:
training : 1
subgraph instance: _no_sens_impl_30 : 00000216313D24C0
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:431/    def _no_sens_impl(self, *inputs):/
subgraph @_no_sens_impl_30 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_11](%para32_inputs) {
  %1(CNode_33) = call @_no_sens_impl_62()
      #scope: (Default)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:437/        if self.return_grad:/
  Return(%1)
      : (<null>)
      #scope: (Default)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:437/        if self.return_grad:/
}
# Order:
#   1: @_no_sens_impl_30:loss{[0]: ValueNode<UnpackCall> MetaFuncGraph-unpack_call.34, [1]: ValueNode<FuncGraph> mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_63, [2]: param_inputs}
#   2: @_no_sens_impl_30:grads{[0]: ValueNode<UnpackGraphPrimitive> UnpackGraph, [1]: ValueNode<FuncGraph> mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_63, [2]: param_inputs}
#   3: @_no_sens_impl_30:grads{[0]: ValueNode<DoSignaturePrimitive> S_Prim_grad, [1]: grads, [2]: CNode_36}
#   4: @_no_sens_impl_30:grads{[0]: ValueNode<UnpackCall> MetaFuncGraph-unpack_call.37, [1]: grads, [2]: param_inputs}
#   5: @_no_sens_impl_30:grads{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Identity_construct_64, [1]: grads}
#   6: @_no_sens_impl_30:CNode_39{[0]: ValueNode<FuncGraph> mindspore_nn_optim_momentum_Momentum_construct_65, [1]: grads}
#   7: @_no_sens_impl_30:loss{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Depend, [1]: loss, [2]: CNode_39}
#   8: @_no_sens_impl_30:CNode_33{[0]: ValueNode<FuncGraph> _no_sens_impl_62}
#   9: @_no_sens_impl_30:CNode_41{[0]: ValueNode<Primitive> Return, [1]: CNode_33}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: mindspore_nn_optim_momentum_Momentum_construct_65 : 00000216313D04E0
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\optim\momentum.py:215/    def construct(self, gradients):/
subgraph @mindspore_nn_optim_momentum_Momentum_construct_65 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_11](%para33_gradients) {
  %1(CNode_66) = S_Prim_AssignAdd[input_names: ["ref", "value"], output_names: ["ref"], side_effect_mem: Bool(1)](%para11_global_step, Tensor(shape=[1], dtype=Int32, value=[1]))
      : (<Ref[Tensor[Int32]], (1), ref_key=:global_step>, <null>) -> (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\optim\momentum.py:223/        self.assignadd(self.global_step, self.global_step_increase_tensor)/
  %2(CNode_67) = StopGradient(%1)
      : (<null>) -> (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\optim\momentum.py:214/    @jit/
  %3(CNode_69) = call @mindspore_nn_optim_momentum_Momentum_construct_68()
      #scope: (Default/optimizer-Momentum)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\optim\momentum.py:224/        if self.use_dist_optimizer:/
  %4(CNode_70) = Depend[side_effect_propagate: I64(1)](%3, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\optim\momentum.py:224/        if self.use_dist_optimizer:/
  Return(%4)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\optim\momentum.py:224/        if self.use_dist_optimizer:/
}
# Order:
#   1: @mindspore_nn_optim_momentum_Momentum_construct_65:gradients{[0]: ValueNode<FuncGraph> flatten_gradients_71, [1]: param_gradients}
#   2: @mindspore_nn_optim_momentum_Momentum_construct_65:gradients{[0]: ValueNode<FuncGraph> decay_weight_72, [1]: gradients}
#   3: @mindspore_nn_optim_momentum_Momentum_construct_65:gradients{[0]: ValueNode<FuncGraph> gradients_centralization_73, [1]: gradients}
#   4: @mindspore_nn_optim_momentum_Momentum_construct_65:gradients{[0]: ValueNode<FuncGraph> scale_grad_74, [1]: gradients}
#   5: @mindspore_nn_optim_momentum_Momentum_construct_65:lr{[0]: ValueNode<FuncGraph> get_lr_75}
#   6: @mindspore_nn_optim_momentum_Momentum_construct_65:CNode_66{[0]: ValueNode<DoSignaturePrimitive> S_Prim_AssignAdd, [1]: param_global_step, [2]: ValueNode<Tensor> Tensor(shape=[1], dtype=Int32, value=[1])}
#   7: @mindspore_nn_optim_momentum_Momentum_construct_65:CNode_69{[0]: ValueNode<FuncGraph> mindspore_nn_optim_momentum_Momentum_construct_68}
#   8: @mindspore_nn_optim_momentum_Momentum_construct_65:CNode_76{[0]: ValueNode<Primitive> Return, [1]: CNode_70}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Identity_construct_64 : 00000216313D1A20
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:505/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Identity_construct_64(%para34_x) {
  Return(%para34_x)
      : (<null>)
      #scope: (Default/grad_reducer-Identity)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:506/        return x/
}
# Order:
#   1: @mindspore_nn_layer_basic_Identity_construct_64:CNode_77{[0]: ValueNode<Primitive> Return, [1]: param_x}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_63 : 00000216313D69D0
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:120/    def construct(self, data, label):/
subgraph @mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_63 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_11](%para35_data, %para36_label) {
  %1(out) = call @__main___LeNet5_construct_78(%para35_data)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:121/        out = self._backbone(data)/
  %2(CNode_46) = call @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_79(%1, %para36_label)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:122/        return self._loss_fn(out, label)/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:122/        return self._loss_fn(out, label)/
}
# Order:
#   1: @mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_63:out{[0]: ValueNode<FuncGraph> __main___LeNet5_construct_78, [1]: param_data}
#   2: @mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_63:CNode_46{[0]: ValueNode<FuncGraph> mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_79, [1]: out, [2]: param_label}
#   3: @mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_63:CNode_47{[0]: ValueNode<Primitive> Return, [1]: CNode_46}


subgraph attr:
training : 1
subgraph instance: _no_sens_impl_62 : 0000021629DFF9A0
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:431/    def _no_sens_impl(self, *inputs):/
subgraph @_no_sens_impl_62 parent: [subgraph @_no_sens_impl_30]() {
  %1(CNode_42) = call @_no_sens_impl_80()
      #scope: (Default)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:437/        if self.return_grad:/
  Return(%1)
      : (<null>)
      #scope: (Default)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:437/        if self.return_grad:/
}
# Order:
#   1: @_no_sens_impl_62:CNode_42{[0]: ValueNode<FuncGraph> _no_sens_impl_80}
#   2: @_no_sens_impl_62:CNode_43{[0]: ValueNode<Primitive> Return, [1]: CNode_42}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: scale_grad_74 : 00000216313D4F40
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:471/    def scale_grad(self, gradients):/
subgraph @scale_grad_74(%para37_gradients) {
  %1(CNode_82) = call @scale_grad_81()
      #scope: (Default/optimizer-Momentum)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:485/        if self.need_scale:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:485/        if self.need_scale:/
}
# Order:
#   1: @scale_grad_74:CNode_82{[0]: ValueNode<FuncGraph> scale_grad_81}
#   2: @scale_grad_74:CNode_83{[0]: ValueNode<Primitive> Return, [1]: CNode_82}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: gradients_centralization_73 : 00000216313D2F60
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:452/    def gradients_centralization(self, gradients):/
subgraph @gradients_centralization_73(%para38_gradients) {
  %1(CNode_85) = call @gradients_centralization_84()
      #scope: (Default/optimizer-Momentum)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:466/        if self.is_group:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:466/        if self.is_group:/
}
# Order:
#   1: @gradients_centralization_73:CNode_85{[0]: ValueNode<FuncGraph> gradients_centralization_84}
#   2: @gradients_centralization_73:CNode_86{[0]: ValueNode<Primitive> Return, [1]: CNode_85}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: decay_weight_72 : 00000216313D2A10
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:429/    def decay_weight(self, gradients):/
subgraph @decay_weight_72(%para39_gradients) {
  %1(CNode_88) = call @decay_weight_87()
      #scope: (Default/optimizer-Momentum)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:442/        if self.exec_weight_decay:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:442/        if self.exec_weight_decay:/
}
# Order:
#   1: @decay_weight_72:CNode_88{[0]: ValueNode<FuncGraph> decay_weight_87}
#   2: @decay_weight_72:CNode_89{[0]: ValueNode<Primitive> Return, [1]: CNode_88}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: flatten_gradients_71 : 00000216313D1F70
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:410/    def flatten_gradients(self, gradients):/
subgraph @flatten_gradients_71(%para40_gradients) {
  %1(CNode_91) = call @flatten_gradients_90()
      #scope: (Default/optimizer-Momentum)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:424/        if self._use_flattened_params:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:424/        if self._use_flattened_params:/
}
# Order:
#   1: @flatten_gradients_71:CNode_91{[0]: ValueNode<FuncGraph> flatten_gradients_90}
#   2: @flatten_gradients_71:CNode_92{[0]: ValueNode<Primitive> Return, [1]: CNode_91}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: get_lr_75 : 00000216313CF4F0
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:739/    def get_lr(self):/
subgraph @get_lr_75 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_11]() {
  %1(CNode_94) = call @get_lr_93()
      #scope: (Default/optimizer-Momentum)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:748/        if self.dynamic_lr:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:748/        if self.dynamic_lr:/
}
# Order:
#   1: @get_lr_75:CNode_94{[0]: ValueNode<FuncGraph> get_lr_93}
#   2: @get_lr_75:CNode_95{[0]: ValueNode<Primitive> Return, [1]: CNode_94}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: mindspore_nn_optim_momentum_Momentum_construct_68 : 00000216313D5F30
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\optim\momentum.py:215/    def construct(self, gradients):/
subgraph @mindspore_nn_optim_momentum_Momentum_construct_68 parent: [subgraph @mindspore_nn_optim_momentum_Momentum_construct_65]() {
  %1(CNode_97) = call @mindspore_nn_optim_momentum_Momentum_construct_96()
      #scope: (Default/optimizer-Momentum)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\optim\momentum.py:234/            if self.is_group_lr:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\optim\momentum.py:234/            if self.is_group_lr:/
}
# Order:
#   1: @mindspore_nn_optim_momentum_Momentum_construct_68:CNode_97{[0]: ValueNode<FuncGraph> mindspore_nn_optim_momentum_Momentum_construct_96}
#   2: @mindspore_nn_optim_momentum_Momentum_construct_68:CNode_98{[0]: ValueNode<Primitive> Return, [1]: CNode_97}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_79 : 0000021629DFF450
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:777/    def construct(self, logits, labels):/
subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_79(%para41_logits, %para42_labels) {
  %1(CNode_48) = S_Prim__check_is_tensor[constexpr_prim: Bool(1)]("logits", %para41_logits, "SoftmaxCrossEntropyWithLogits")
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:778/        _check_is_tensor('logits', logits, self.cls_name)/
  %2(CNode_49) = S_Prim__check_is_tensor[constexpr_prim: Bool(1)]("labels", %para42_labels, "SoftmaxCrossEntropyWithLogits")
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:779/        _check_is_tensor('labels', labels, self.cls_name)/
  %3(CNode_50) = MakeTuple(%1, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:777/    def construct(self, logits, labels):/
  %4(CNode_51) = StopGradient(%3)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:777/    def construct(self, logits, labels):/
  %5(CNode_52) = call @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_99()
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:780/        if self.sparse:/
  %6(CNode_53) = Depend[side_effect_propagate: I64(1)](%5, %4)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:780/        if self.sparse:/
  Return(%6)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:780/        if self.sparse:/
}
# Order:
#   1: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_79:CNode_48{[0]: ValueNode<DoSignaturePrimitive> S_Prim__check_is_tensor, [1]: ValueNode<StringImm> logits, [2]: param_logits, [3]: ValueNode<StringImm> SoftmaxCrossEntropyWithLogits}
#   2: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_79:CNode_49{[0]: ValueNode<DoSignaturePrimitive> S_Prim__check_is_tensor, [1]: ValueNode<StringImm> labels, [2]: param_labels, [3]: ValueNode<StringImm> SoftmaxCrossEntropyWithLogits}
#   3: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_79:CNode_52{[0]: ValueNode<FuncGraph> mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_99}
#   4: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_79:CNode_54{[0]: ValueNode<Primitive> Return, [1]: CNode_53}


subgraph attr:
training : 1
subgraph instance: __main___LeNet5_construct_78 : 00000216313D6F20
# In file E:\mindspore\mydemo.py:40/    def construct(self, x):/
subgraph @__main___LeNet5_construct_78 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_11](%para43_x) {
  %1(CNode_101) = call @mindspore_nn_layer_conv_Conv2d_construct_100(%para43_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file E:\mindspore\mydemo.py:41/        x = self.max_pool2d(self.relu(self.conv1(x)))/
  %2(CNode_103) = call @mindspore_nn_layer_activation_ReLU_construct_102(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file E:\mindspore\mydemo.py:41/        x = self.max_pool2d(self.relu(self.conv1(x)))/
  %3(x) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_104(%2)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file E:\mindspore\mydemo.py:41/        x = self.max_pool2d(self.relu(self.conv1(x)))/
  %4(CNode_106) = call @mindspore_nn_layer_conv_Conv2d_construct_105(%3)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file E:\mindspore\mydemo.py:42/        x = self.max_pool2d(self.relu(self.conv2(x)))/
  %5(CNode_107) = call @mindspore_nn_layer_activation_ReLU_construct_102(%4)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file E:\mindspore\mydemo.py:42/        x = self.max_pool2d(self.relu(self.conv2(x)))/
  %6(x) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_104(%5)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file E:\mindspore\mydemo.py:42/        x = self.max_pool2d(self.relu(self.conv2(x)))/
  %7(x) = call @mindspore_nn_layer_basic_Flatten_construct_108(%6)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file E:\mindspore\mydemo.py:43/        x = self.flatten(x)/
  %8(CNode_110) = call @mindspore_nn_layer_basic_Dense_construct_109(%7)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file E:\mindspore\mydemo.py:44/        x = self.relu(self.fc1(x))/
  %9(x) = call @mindspore_nn_layer_activation_ReLU_construct_102(%8)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file E:\mindspore\mydemo.py:44/        x = self.relu(self.fc1(x))/
  %10(CNode_112) = call @mindspore_nn_layer_basic_Dense_construct_111(%9)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file E:\mindspore\mydemo.py:45/        x = self.relu(self.fc2(x))/
  %11(x) = call @mindspore_nn_layer_activation_ReLU_construct_102(%10)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file E:\mindspore\mydemo.py:45/        x = self.relu(self.fc2(x))/
  %12(x) = call @mindspore_nn_layer_basic_Dense_construct_113(%11)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file E:\mindspore\mydemo.py:46/        x = self.fc3(x)/
  Return(%12)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file E:\mindspore\mydemo.py:47/        return x/
}
# Order:
#   1: @__main___LeNet5_construct_78:CNode_101{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_100, [1]: param_x}
#   2: @__main___LeNet5_construct_78:CNode_103{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_102, [1]: CNode_101}
#   3: @__main___LeNet5_construct_78:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_104, [1]: CNode_103}
#   4: @__main___LeNet5_construct_78:CNode_106{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_105, [1]: x}
#   5: @__main___LeNet5_construct_78:CNode_107{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_102, [1]: CNode_106}
#   6: @__main___LeNet5_construct_78:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_104, [1]: CNode_107}
#   7: @__main___LeNet5_construct_78:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Flatten_construct_108, [1]: x}
#   8: @__main___LeNet5_construct_78:CNode_110{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dense_construct_109, [1]: x}
#   9: @__main___LeNet5_construct_78:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_102, [1]: CNode_110}
#  10: @__main___LeNet5_construct_78:CNode_112{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dense_construct_111, [1]: x}
#  11: @__main___LeNet5_construct_78:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_activation_ReLU_construct_102, [1]: CNode_112}
#  12: @__main___LeNet5_construct_78:x{[0]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Dense_construct_113, [1]: x}
#  13: @__main___LeNet5_construct_78:CNode_114{[0]: ValueNode<Primitive> Return, [1]: x}


subgraph attr:
training : 1
subgraph instance: _no_sens_impl_80 : 0000021629E04950
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:431/    def _no_sens_impl(self, *inputs):/
subgraph @_no_sens_impl_80 parent: [subgraph @_no_sens_impl_30]() {
  %1(loss) = $(_no_sens_impl_30):UnpackCall_unpack_call(@mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_63, %para32_inputs)
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:433/        loss = self.network(*inputs)/
  %2(grads) = $(_no_sens_impl_30):UnpackGraph(@mindspore_nn_wrap_cell_wrapper_WithLossCell_construct_63, %para32_inputs)
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:434/        grads = self.grad_no_sens(self.network, self.weights)(*inputs)/
  %3(CNode_36) = $(_no_sens_impl_30):MakeTuple(%para3_conv1.weight, %para4_conv2.weight, %para5_fc1.weight, %para6_fc1.bias, %para7_fc2.weight, %para8_fc2.bias, %para9_fc3.weight, %para10_fc3.bias)
      : (<Ref[Tensor[Float32]], (6, 1, 5, 5), ref_key=:conv1.weight>, <Ref[Tensor[Float32]], (16, 6, 5, 5), ref_key=:conv2.weight>, <Ref[Tensor[Float32]], (120, 784), ref_key=:fc1.weight>, <Ref[Tensor[Float32]], (120), ref_key=:fc1.bias>, <Ref[Tensor[Float32]], (84, 120), ref_key=:fc2.weight>, <Ref[Tensor[Float32]], (84), ref_key=:fc2.bias>, <Ref[Tensor[Float32]], (10, 84), ref_key=:fc3.weight>, <Ref[Tensor[Float32]], (10), ref_key=:fc3.bias>) -> (<null>)
      #scope: (Default)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:434/        grads = self.grad_no_sens(self.network, self.weights)(*inputs)/
  %4(grads) = $(_no_sens_impl_30):S_Prim_grad(%2, %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:434/        grads = self.grad_no_sens(self.network, self.weights)(*inputs)/
  %5(grads) = $(_no_sens_impl_30):UnpackCall_unpack_call(%4, %para32_inputs)
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:434/        grads = self.grad_no_sens(self.network, self.weights)(*inputs)/
  %6(grads) = $(_no_sens_impl_30):call @mindspore_nn_layer_basic_Identity_construct_64(%5)
      : (<null>) -> (<null>)
      #scope: (Default)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:435/        grads = self.grad_reducer(grads)/
  %7(CNode_39) = $(_no_sens_impl_30):call @mindspore_nn_optim_momentum_Momentum_construct_65(%6)
      : (<null>) -> (<null>)
      #scope: (Default)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:436/        loss = F.depend(loss, self.optimizer(grads))/
  %8(loss) = $(_no_sens_impl_30):S_Prim_Depend[side_effect_propagate: I64(1)](%1, %7)
      : (<null>, <null>) -> (<null>)
      #scope: (Default)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:436/        loss = F.depend(loss, self.optimizer(grads))/
  Return(%8)
      : (<null>)
      #scope: (Default)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\wrap\cell_wrapper.py:442/        return loss/
}
# Order:
#   1: @_no_sens_impl_80:CNode_44{[0]: ValueNode<Primitive> Return, [1]: loss}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: scale_grad_81 : 00000216313D59E0
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:471/    def scale_grad(self, gradients):/
subgraph @scale_grad_81 parent: [subgraph @scale_grad_74]() {
  %1(CNode_116) = call @scale_grad_115()
      #scope: (Default/optimizer-Momentum)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:485/        if self.need_scale:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:485/        if self.need_scale:/
}
# Order:
#   1: @scale_grad_81:CNode_116{[0]: ValueNode<FuncGraph> scale_grad_115}
#   2: @scale_grad_81:CNode_117{[0]: ValueNode<Primitive> Return, [1]: CNode_116}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: gradients_centralization_84 : 00000216313CEFA0
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:452/    def gradients_centralization(self, gradients):/
subgraph @gradients_centralization_84 parent: [subgraph @gradients_centralization_73]() {
  %1(CNode_119) = call @gradients_centralization_118()
      #scope: (Default/optimizer-Momentum)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:466/        if self.is_group:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:466/        if self.is_group:/
}
# Order:
#   1: @gradients_centralization_84:CNode_119{[0]: ValueNode<FuncGraph> gradients_centralization_118}
#   2: @gradients_centralization_84:CNode_120{[0]: ValueNode<Primitive> Return, [1]: CNode_119}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: decay_weight_87 : 00000216313D0F80
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:429/    def decay_weight(self, gradients):/
subgraph @decay_weight_87 parent: [subgraph @decay_weight_72]() {
  %1(CNode_122) = call @decay_weight_121()
      #scope: (Default/optimizer-Momentum)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:442/        if self.exec_weight_decay:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:442/        if self.exec_weight_decay:/
}
# Order:
#   1: @decay_weight_87:CNode_122{[0]: ValueNode<FuncGraph> decay_weight_121}
#   2: @decay_weight_87:CNode_123{[0]: ValueNode<Primitive> Return, [1]: CNode_122}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: flatten_gradients_90 : 00000216313D44A0
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:410/    def flatten_gradients(self, gradients):/
subgraph @flatten_gradients_90 parent: [subgraph @flatten_gradients_71]() {
  %1(CNode_125) = call @flatten_gradients_124()
      #scope: (Default/optimizer-Momentum)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:424/        if self._use_flattened_params:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:424/        if self._use_flattened_params:/
}
# Order:
#   1: @flatten_gradients_90:CNode_125{[0]: ValueNode<FuncGraph> flatten_gradients_124}
#   2: @flatten_gradients_90:CNode_126{[0]: ValueNode<Primitive> Return, [1]: CNode_125}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: get_lr_93 : 00000216313D3A00
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:739/    def get_lr(self):/
subgraph @get_lr_93 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_11]() {
  %1(CNode_128) = call @get_lr_127()
      #scope: (Default/optimizer-Momentum)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:748/        if self.dynamic_lr:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:748/        if self.dynamic_lr:/
}
# Order:
#   1: @get_lr_93:CNode_128{[0]: ValueNode<FuncGraph> get_lr_127}
#   2: @get_lr_93:CNode_129{[0]: ValueNode<Primitive> Return, [1]: CNode_128}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: mindspore_nn_optim_momentum_Momentum_construct_96 : 00000216313CFA40
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\optim\momentum.py:215/    def construct(self, gradients):/
subgraph @mindspore_nn_optim_momentum_Momentum_construct_96 parent: [subgraph @mindspore_nn_optim_momentum_Momentum_construct_65]() {
  %1(CNode_131) = call @mindspore_nn_optim_momentum_Momentum_construct_130()
      #scope: (Default/optimizer-Momentum)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\optim\momentum.py:238/                success = self.hyper_map_reverse(F.partial(_momentum_opt, self.opt, self.momentum, lr),/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\optim\momentum.py:238/                success = self.hyper_map_reverse(F.partial(_momentum_opt, self.opt, self.momentum, lr),/
}
# Order:
#   1: @mindspore_nn_optim_momentum_Momentum_construct_96:CNode_132{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Partial, [1]: ValueNode<DoSignaturePrimitive> S_Prim_momentum_opt, [2]: ValueNode<DoSignaturePrimitive> S_Prim_ApplyMomentum, [3]: param_momentum, [4]: lr}
#   2: @mindspore_nn_optim_momentum_Momentum_construct_96:success{[0]: ValueNode<DoSignaturePrimitive> S_Prim_hyper_map, [1]: CNode_132, [2]: gradients, [3]: CNode_133, [4]: CNode_134, [5]: ValueNode<ValueTuple> (false, false, false, false, false, false, false, false), [6]: ValueNode<ValueTuple> (false, false, false, false, false, false, false, false)}
#   3: @mindspore_nn_optim_momentum_Momentum_construct_96:CNode_131{[0]: ValueNode<FuncGraph> mindspore_nn_optim_momentum_Momentum_construct_130}
#   4: @mindspore_nn_optim_momentum_Momentum_construct_96:CNode_135{[0]: ValueNode<Primitive> Return, [1]: CNode_131}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_99 : 0000021629E04EA0
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:777/    def construct(self, logits, labels):/
subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_99 parent: [subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_79]() {
  %1(CNode_55) = S_Prim_equal("mean", "mean")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:781/            if self.reduction == 'mean':/
  %2(CNode_56) = Cond(%1, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:781/            if self.reduction == 'mean':/
  %3(CNode_57) = Switch(%2, @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_136, @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_137)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:781/            if self.reduction == 'mean':/
  %4(CNode_59) = %3()
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:781/            if self.reduction == 'mean':/
  Return(%4)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:781/            if self.reduction == 'mean':/
}
# Order:
#   1: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_99:CNode_55{[0]: ValueNode<DoSignaturePrimitive> S_Prim_equal, [1]: ValueNode<StringImm> mean, [2]: ValueNode<StringImm> mean}
#   2: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_99:CNode_56{[0]: ValueNode<Primitive> Cond, [1]: CNode_55, [2]: ValueNode<BoolImm> false}
#   3: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_99:CNode_57{[0]: ValueNode<Primitive> Switch, [1]: CNode_56, [2]: ValueNode<FuncGraph> mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_136, [3]: ValueNode<FuncGraph> mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_137}
#   4: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_99:CNode_59{[0]: CNode_57}
#   5: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_99:CNode_60{[0]: ValueNode<Primitive> Return, [1]: CNode_59}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dense_construct_113 : 0000021629E03960
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dense_construct_113 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_11](%para44_x) {
  %1(CNode_139) = call @L_mindspore_nn_layer_basic_Dense_construct_138(%para44_x, %para10_fc3.bias, %para9_fc3.weight)
      : (<null>, <Ref[Tensor[Float32]], (10), ref_key=:fc3.bias>, <Ref[Tensor[Float32]], (10, 84), ref_key=:fc3.weight>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:625/        if len(x_shape) != 2:/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dense_construct_113:CNode_139{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_basic_Dense_construct_138, [1]: param_x, [2]: param_fc3.bias, [3]: param_fc3.weight}
#   2: @mindspore_nn_layer_basic_Dense_construct_113:CNode_140{[0]: ValueNode<Primitive> Return, [1]: CNode_139}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_activation_ReLU_construct_102 : 0000021629E02420
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\activation.py:437/    def construct(self, x):/
subgraph @mindspore_nn_layer_activation_ReLU_construct_102(%para45_x) {
  %1(CNode_141) = S_Prim_ReLU[input_names: ["x"], output_names: ["output"]](%para45_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/relu-ReLU)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\activation.py:438/        return self.relu(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/relu-ReLU)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\activation.py:438/        return self.relu(x)/
}
# Order:
#   1: @mindspore_nn_layer_activation_ReLU_construct_102:CNode_141{[0]: ValueNode<DoSignaturePrimitive> S_Prim_ReLU, [1]: param_x}
#   2: @mindspore_nn_layer_activation_ReLU_construct_102:CNode_142{[0]: ValueNode<Primitive> Return, [1]: CNode_141}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dense_construct_111 : 0000021629E00EE0
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dense_construct_111 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_11](%para46_x) {
  %1(CNode_143) = call @L_mindspore_nn_layer_basic_Dense_construct_138(%para46_x, %para8_fc2.bias, %para7_fc2.weight)
      : (<null>, <Ref[Tensor[Float32]], (84), ref_key=:fc2.bias>, <Ref[Tensor[Float32]], (84, 120), ref_key=:fc2.weight>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc2-Dense)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:625/        if len(x_shape) != 2:/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dense_construct_111:CNode_143{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_basic_Dense_construct_138, [1]: param_x, [2]: param_fc2.bias, [3]: param_fc2.weight}
#   2: @mindspore_nn_layer_basic_Dense_construct_111:CNode_144{[0]: ValueNode<Primitive> Return, [1]: CNode_143}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Dense_construct_109 : 0000021629DF9A00
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Dense_construct_109 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_11](%para47_x) {
  %1(CNode_145) = call @L_mindspore_nn_layer_basic_Dense_construct_138(%para47_x, %para6_fc1.bias, %para5_fc1.weight)
      : (<null>, <Ref[Tensor[Float32]], (120), ref_key=:fc1.bias>, <Ref[Tensor[Float32]], (120, 784), ref_key=:fc1.weight>) -> (<null>)
      #scope: (Default)
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc1-Dense)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:625/        if len(x_shape) != 2:/
}
# Order:
#   1: @mindspore_nn_layer_basic_Dense_construct_109:CNode_145{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_basic_Dense_construct_138, [1]: param_x, [2]: param_fc1.bias, [3]: param_fc1.weight}
#   2: @mindspore_nn_layer_basic_Dense_construct_109:CNode_146{[0]: ValueNode<Primitive> Return, [1]: CNode_145}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Flatten_construct_108 : 00000216313DEEA0
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:461/    def construct(self, x):/
subgraph @mindspore_nn_layer_basic_Flatten_construct_108(%para48_x) {
  %1(x_rank) = call @rank_147(%para48_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:462/        x_rank = F.rank(x)/
  %2(CNode_148) = S_Prim_not_equal(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:463/        ndim = x_rank if x_rank != 0 else 1/
  %3(CNode_149) = Cond(%2, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:463/        ndim = x_rank if x_rank != 0 else 1/
  %4(CNode_150) = Switch(%3, @mindspore_nn_layer_basic_Flatten_construct_151, @mindspore_nn_layer_basic_Flatten_construct_152)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:463/        ndim = x_rank if x_rank != 0 else 1/
  %5(ndim) = %4()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:463/        ndim = x_rank if x_rank != 0 else 1/
  %6(CNode_154) = call @check_axis_valid_153(I64(1), %5)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:464/        self.check_axis_valid(self.start_dim, ndim)/
  %7(CNode_155) = call @check_axis_valid_153(I64(-1), %5)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:465/        self.check_axis_valid(self.end_dim, ndim)/
  %8(CNode_156) = MakeTuple(%6, %7)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:461/    def construct(self, x):/
  %9(CNode_157) = StopGradient(%8)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:461/    def construct(self, x):/
  %10(CNode_158) = S_Prim_MakeTuple(%para48_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  %11(CNode_159) = S_Prim_MakeTuple("start_dim", "end_dim")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  %12(CNode_160) = S_Prim_MakeTuple(I64(1), I64(-1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  %13(CNode_161) = S_Prim_make_dict(%11, %12)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  %14(CNode_162) = UnpackCall_unpack_call(@flatten_163, %10, %13)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  %15(CNode_164) = Depend[side_effect_propagate: I64(1)](%14, %9)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  Return(%15)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
}
# Order:
#   1: @mindspore_nn_layer_basic_Flatten_construct_108:x_rank{[0]: ValueNode<FuncGraph> rank_147, [1]: param_x}
#   2: @mindspore_nn_layer_basic_Flatten_construct_108:CNode_148{[0]: ValueNode<DoSignaturePrimitive> S_Prim_not_equal, [1]: x_rank, [2]: ValueNode<Int64Imm> 0}
#   3: @mindspore_nn_layer_basic_Flatten_construct_108:CNode_149{[0]: ValueNode<Primitive> Cond, [1]: CNode_148, [2]: ValueNode<BoolImm> false}
#   4: @mindspore_nn_layer_basic_Flatten_construct_108:CNode_150{[0]: ValueNode<Primitive> Switch, [1]: CNode_149, [2]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Flatten_construct_151, [3]: ValueNode<FuncGraph> mindspore_nn_layer_basic_Flatten_construct_152}
#   5: @mindspore_nn_layer_basic_Flatten_construct_108:ndim{[0]: CNode_150}
#   6: @mindspore_nn_layer_basic_Flatten_construct_108:CNode_154{[0]: ValueNode<FuncGraph> check_axis_valid_153, [1]: ValueNode<Int64Imm> 1, [2]: ndim}
#   7: @mindspore_nn_layer_basic_Flatten_construct_108:CNode_155{[0]: ValueNode<FuncGraph> check_axis_valid_153, [1]: ValueNode<Int64Imm> -1, [2]: ndim}
#   8: @mindspore_nn_layer_basic_Flatten_construct_108:CNode_158{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: param_x}
#   9: @mindspore_nn_layer_basic_Flatten_construct_108:CNode_159{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: ValueNode<StringImm> start_dim, [2]: ValueNode<StringImm> end_dim}
#  10: @mindspore_nn_layer_basic_Flatten_construct_108:CNode_160{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: ValueNode<Int64Imm> 1, [2]: ValueNode<Int64Imm> -1}
#  11: @mindspore_nn_layer_basic_Flatten_construct_108:CNode_161{[0]: ValueNode<DoSignaturePrimitive> S_Prim_make_dict, [1]: CNode_159, [2]: CNode_160}
#  12: @mindspore_nn_layer_basic_Flatten_construct_108:CNode_162{[0]: ValueNode<UnpackCall> MetaFuncGraph-unpack_call.165, [1]: ValueNode<FuncGraph> flatten_163, [2]: CNode_158, [3]: CNode_161}
#  13: @mindspore_nn_layer_basic_Flatten_construct_108:CNode_166{[0]: ValueNode<Primitive> Return, [1]: CNode_164}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_104 : 00000216313DCEC0
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_104(%para49_x) {
  %1(CNode_167) = getattr(%para49_x, "ndim")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:570/        if x.ndim == 3:/
  %2(CNode_168) = S_Prim_equal(%1, I64(3))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:570/        if x.ndim == 3:/
  %3(CNode_169) = Cond(%2, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:570/        if x.ndim == 3:/
  %4(CNode_170) = Switch(%3, @mindspore_nn_layer_pooling_MaxPool2d_construct_171, @mindspore_nn_layer_pooling_MaxPool2d_construct_172)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:570/        if x.ndim == 3:/
  %5(CNode_173) = %4()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:570/        if x.ndim == 3:/
  Return(%5)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:570/        if x.ndim == 3:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_104:CNode_167{[0]: ValueNode<Primitive> getattr, [1]: param_x, [2]: ValueNode<StringImm> ndim}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_104:CNode_168{[0]: ValueNode<DoSignaturePrimitive> S_Prim_equal, [1]: CNode_167, [2]: ValueNode<Int64Imm> 3}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_104:CNode_169{[0]: ValueNode<Primitive> Cond, [1]: CNode_168, [2]: ValueNode<BoolImm> false}
#   4: @mindspore_nn_layer_pooling_MaxPool2d_construct_104:CNode_170{[0]: ValueNode<Primitive> Switch, [1]: CNode_169, [2]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_171, [3]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_172}
#   5: @mindspore_nn_layer_pooling_MaxPool2d_construct_104:CNode_173{[0]: CNode_170}
#   6: @mindspore_nn_layer_pooling_MaxPool2d_construct_104:CNode_174{[0]: ValueNode<Primitive> Return, [1]: CNode_173}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_105 : 00000216313DC420
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_105 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_11](%para50_x) {
  %1(CNode_176) = call @mindspore_nn_layer_conv_Conv2d_construct_175()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/conv2-Conv2d)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/conv2-Conv2d)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_105:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_conv2.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_105:CNode_176{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_175}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_105:CNode_177{[0]: ValueNode<Primitive> Return, [1]: CNode_176}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_100 : 00000216313D8F00
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_100 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_11](%para51_x) {
  %1(CNode_179) = call @mindspore_nn_layer_conv_Conv2d_construct_178()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/conv1-Conv2d)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/conv1-Conv2d)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_100:output{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Conv2D, [1]: param_x, [2]: param_conv1.weight}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_100:CNode_179{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_178}
#   3: @mindspore_nn_layer_conv_Conv2d_construct_100:CNode_180{[0]: ValueNode<Primitive> Return, [1]: CNode_179}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: scale_grad_115 : 00000216313CFF90
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:471/    def scale_grad(self, gradients):/
subgraph @scale_grad_115 parent: [subgraph @scale_grad_74]() {
  Return(%para37_gradients)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:488/        return gradients/
}
# Order:
#   1: @scale_grad_115:CNode_181{[0]: ValueNode<Primitive> Return, [1]: param_gradients}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: gradients_centralization_118 : 00000216313D34B0
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:452/    def gradients_centralization(self, gradients):/
subgraph @gradients_centralization_118 parent: [subgraph @gradients_centralization_73]() {
  Return(%para38_gradients)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:469/        return gradients/
}
# Order:
#   1: @gradients_centralization_118:CNode_182{[0]: ValueNode<Primitive> Return, [1]: param_gradients}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: decay_weight_121 : 00000216313D5490
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:429/    def decay_weight(self, gradients):/
subgraph @decay_weight_121 parent: [subgraph @decay_weight_72]() {
  Return(%para39_gradients)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:450/        return gradients/
}
# Order:
#   1: @decay_weight_121:CNode_183{[0]: ValueNode<Primitive> Return, [1]: param_gradients}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: flatten_gradients_124 : 00000216313D0A30
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:410/    def flatten_gradients(self, gradients):/
subgraph @flatten_gradients_124 parent: [subgraph @flatten_gradients_71]() {
  Return(%para40_gradients)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:427/        return gradients/
}
# Order:
#   1: @flatten_gradients_124:CNode_184{[0]: ValueNode<Primitive> Return, [1]: param_gradients}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: get_lr_127 : 00000216313D49F0
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:739/    def get_lr(self):/
subgraph @get_lr_127 parent: [subgraph @mindspore_nn_wrap_cell_wrapper_TrainOneStepCell_construct_11]() {
  Return(%para21_learning_rate)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\optim\optimizer.py:756/        return lr/
}
# Order:
#   1: @get_lr_127:CNode_185{[0]: ValueNode<Primitive> Return, [1]: param_learning_rate}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: mindspore_nn_optim_momentum_Momentum_construct_130 : 00000216313CE500
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\optim\momentum.py:215/    def construct(self, gradients):/
subgraph @mindspore_nn_optim_momentum_Momentum_construct_130 parent: [subgraph @mindspore_nn_optim_momentum_Momentum_construct_96]() {
  %1(CNode_187) = call @mindspore_nn_optim_momentum_Momentum_construct_186()
      #scope: (Default/optimizer-Momentum)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\optim\momentum.py:234/            if self.is_group_lr:/
  Return(%1)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\optim\momentum.py:234/            if self.is_group_lr:/
}
# Order:
#   1: @mindspore_nn_optim_momentum_Momentum_construct_130:CNode_187{[0]: ValueNode<FuncGraph> mindspore_nn_optim_momentum_Momentum_construct_186}
#   2: @mindspore_nn_optim_momentum_Momentum_construct_130:CNode_188{[0]: ValueNode<Primitive> Return, [1]: CNode_187}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_136 : 0000021629DFDF10
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:777/    def construct(self, logits, labels):/
subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_136 parent: [subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_79]() {
  %1(x) = S_Prim_SparseSoftmaxCrossEntropyWithLogits[is_grad: Bool(0), sens: F32(1), input_names: ["features", "labels"], output_names: ["output"]](%para41_logits, %para42_labels)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:782/                x = self.sparse_softmax_cross_entropy(logits, labels)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:783/                return x/
}
# Order:
#   1: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_136:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_SparseSoftmaxCrossEntropyWithLogits, [1]: param_logits, [2]: param_labels}
#   2: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_136:CNode_61{[0]: ValueNode<Primitive> Return, [1]: x}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_137 : 0000021629E02970
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:777/    def construct(self, logits, labels):/
subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_137 parent: [subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_79]() {
  %1(CNode_190) = call @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_189()
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:781/            if self.reduction == 'mean':/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:781/            if self.reduction == 'mean':/
}
# Order:
#   1: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_137:CNode_190{[0]: ValueNode<FuncGraph> mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_189}
#   2: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_137:CNode_191{[0]: ValueNode<Primitive> Return, [1]: CNode_190}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_basic_Dense_construct_138 : 0000021629DF1FD0
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_basic_Dense_construct_138(%para52_x, %para53_, %para54_) {
  %1(x_shape) = S_Prim_Shape(%para52_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:623/        x_shape = self.shape_op(x)/
  %2(CNode_192) = S_Prim_check_dense_input_shape[constexpr_prim: Bool(1)](%1, "Dense")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:624/        check_dense_input_shape(x_shape, self.cls_name)/
  %3(CNode_193) = StopGradient(%2)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
  %4(CNode_194) = S_Prim_inner_len(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:625/        if len(x_shape) != 2:/
  %5(CNode_195) = S_Prim_not_equal(%4, I64(2))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:625/        if len(x_shape) != 2:/
  %6(CNode_196) = Cond(%5, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:625/        if len(x_shape) != 2:/
  %7(CNode_197) = Switch(%6, @L_mindspore_nn_layer_basic_Dense_construct_198, @L_mindspore_nn_layer_basic_Dense_construct_199)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:625/        if len(x_shape) != 2:/
  %8(CNode_200) = %7()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:625/        if len(x_shape) != 2:/
  %9(CNode_202) = call @L_mindspore_nn_layer_basic_Dense_construct_201(%8)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file E:\mindspore\mydemo.py:46/        x = self.fc3(x)/
  %10(CNode_203) = Depend[side_effect_propagate: I64(1)](%9, %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file E:\mindspore\mydemo.py:46/        x = self.fc3(x)/
  Return(%10)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:625/        if len(x_shape) != 2:/
}
# Order:
#   1: @L_mindspore_nn_layer_basic_Dense_construct_138:x_shape{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Shape, [1]: param_x}
#   2: @L_mindspore_nn_layer_basic_Dense_construct_138:CNode_192{[0]: ValueNode<DoSignaturePrimitive> S_Prim_check_dense_input_shape, [1]: x_shape, [2]: ValueNode<StringImm> Dense}
#   3: @L_mindspore_nn_layer_basic_Dense_construct_138:CNode_194{[0]: ValueNode<DoSignaturePrimitive> S_Prim_inner_len, [1]: x_shape}
#   4: @L_mindspore_nn_layer_basic_Dense_construct_138:CNode_195{[0]: ValueNode<DoSignaturePrimitive> S_Prim_not_equal, [1]: CNode_194, [2]: ValueNode<Int64Imm> 2}
#   5: @L_mindspore_nn_layer_basic_Dense_construct_138:CNode_196{[0]: ValueNode<Primitive> Cond, [1]: CNode_195, [2]: ValueNode<BoolImm> false}
#   6: @L_mindspore_nn_layer_basic_Dense_construct_138:CNode_197{[0]: ValueNode<Primitive> Switch, [1]: CNode_196, [2]: ValueNode<FuncGraph> L_mindspore_nn_layer_basic_Dense_construct_198, [3]: ValueNode<FuncGraph> L_mindspore_nn_layer_basic_Dense_construct_199}
#   7: @L_mindspore_nn_layer_basic_Dense_construct_138:CNode_200{[0]: CNode_197}
#   8: @L_mindspore_nn_layer_basic_Dense_construct_138:CNode_202{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_basic_Dense_construct_201, [1]: CNode_200}
#   9: @L_mindspore_nn_layer_basic_Dense_construct_138:CNode_203{[0]: ValueNode<Primitive> Depend, [1]: CNode_202, [2]: CNode_193}
#  10: @L_mindspore_nn_layer_basic_Dense_construct_138:CNode_140{[0]: ValueNode<Primitive> Return, [1]: CNode_203}


subgraph attr:
training : 1
subgraph instance: check_axis_valid_153 : 00000216313D79C0
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:457/    def check_axis_valid(self, axis, ndim):/
subgraph @check_axis_valid_153(%para55_axis, %para56_ndim) {
  %1(CNode_204) = S_Prim_negative(%para56_ndim)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:458/        if axis < -ndim or axis >= ndim:/
  %2(CNode_205) = S_Prim_less(%para55_axis, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:458/        if axis < -ndim or axis >= ndim:/
  %3(CNode_206) = Cond(%2, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:458/        if axis < -ndim or axis >= ndim:/
  %4(CNode_207) = Switch(%3, @check_axis_valid_208, @check_axis_valid_209)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:458/        if axis < -ndim or axis >= ndim:/
  %5(CNode_210) = %4()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:458/        if axis < -ndim or axis >= ndim:/
  %6(CNode_211) = Cond(%5, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:458/        if axis < -ndim or axis >= ndim:/
  %7(CNode_212) = Switch(%6, @check_axis_valid_213, @check_axis_valid_214)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:458/        if axis < -ndim or axis >= ndim:/
  %8(CNode_215) = %7()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:458/        if axis < -ndim or axis >= ndim:/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:458/        if axis < -ndim or axis >= ndim:/
}
# Order:
#   1: @check_axis_valid_153:CNode_204{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: param_ndim}
#   2: @check_axis_valid_153:CNode_205{[0]: ValueNode<DoSignaturePrimitive> S_Prim_less, [1]: param_axis, [2]: CNode_204}
#   3: @check_axis_valid_153:CNode_206{[0]: ValueNode<Primitive> Cond, [1]: CNode_205, [2]: ValueNode<BoolImm> false}
#   4: @check_axis_valid_153:CNode_207{[0]: ValueNode<Primitive> Switch, [1]: CNode_206, [2]: ValueNode<FuncGraph> check_axis_valid_208, [3]: ValueNode<FuncGraph> check_axis_valid_209}
#   5: @check_axis_valid_153:CNode_210{[0]: CNode_207}
#   6: @check_axis_valid_153:CNode_211{[0]: ValueNode<Primitive> Cond, [1]: CNode_210, [2]: ValueNode<BoolImm> false}
#   7: @check_axis_valid_153:CNode_212{[0]: ValueNode<Primitive> Switch, [1]: CNode_211, [2]: ValueNode<FuncGraph> check_axis_valid_213, [3]: ValueNode<FuncGraph> check_axis_valid_214}
#   8: @check_axis_valid_153:CNode_215{[0]: CNode_212}
#   9: @check_axis_valid_153:CNode_216{[0]: ValueNode<Primitive> Return, [1]: CNode_215}


subgraph attr:
subgraph instance: rank_147 : 0000021629DF8F60
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1541/def rank(input_x):/
subgraph @rank_147(%para57_input_x) {
  %1(CNode_217) = S_Prim_Rank(%para57_input_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1571/    return rank_(input_x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1571/    return rank_(input_x)/
}
# Order:
#   1: @rank_147:CNode_217{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Rank, [1]: param_input_x}
#   2: @rank_147:CNode_218{[0]: ValueNode<Primitive> Return, [1]: CNode_217}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Flatten_construct_151 : 0000021629DF94B0
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:463/        ndim = x_rank if x_rank != 0 else 1/
subgraph @mindspore_nn_layer_basic_Flatten_construct_151 parent: [subgraph @mindspore_nn_layer_basic_Flatten_construct_108]() {
  %1(x_rank) = $(mindspore_nn_layer_basic_Flatten_construct_108):call @rank_147(%para48_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:462/        x_rank = F.rank(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:463/        ndim = x_rank if x_rank != 0 else 1/
}
# Order:
#   1: @mindspore_nn_layer_basic_Flatten_construct_151:CNode_219{[0]: ValueNode<Primitive> Return, [1]: x_rank}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_basic_Flatten_construct_152 : 0000021629DF1530
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:463/        ndim = x_rank if x_rank != 0 else 1/
subgraph @mindspore_nn_layer_basic_Flatten_construct_152() {
  Return(I64(1))
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:463/        ndim = x_rank if x_rank != 0 else 1/
}
# Order:
#   1: @mindspore_nn_layer_basic_Flatten_construct_152:CNode_220{[0]: ValueNode<Primitive> Return, [1]: ValueNode<Int64Imm> 1}


subgraph attr:
subgraph instance: flatten_163 : 00000216313DFE90
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_163(%para58_input, %para59_order, %para60_start_dim, %para61_end_dim) {
  %1(CNode_221) = S_Prim_isinstance(%para58_input, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1731/    if not isinstance(input, Tensor):/
  %2(CNode_222) = S_Prim_logical_not(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1731/    if not isinstance(input, Tensor):/
  %3(CNode_223) = Cond(%2, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1731/    if not isinstance(input, Tensor):/
  %4(CNode_224) = Switch(%3, @flatten_225, @flatten_226)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1731/    if not isinstance(input, Tensor):/
  %5(CNode_227) = %4()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1731/    if not isinstance(input, Tensor):/
  Return(%5)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1731/    if not isinstance(input, Tensor):/
}
# Order:
#   1: @flatten_163:CNode_221{[0]: ValueNode<DoSignaturePrimitive> S_Prim_isinstance, [1]: param_input, [2]: ValueNode<ClassType> class 'mindspore.common.tensor.Tensor'}
#   2: @flatten_163:CNode_222{[0]: ValueNode<DoSignaturePrimitive> S_Prim_logical_not, [1]: CNode_221}
#   3: @flatten_163:CNode_223{[0]: ValueNode<Primitive> Cond, [1]: CNode_222, [2]: ValueNode<BoolImm> false}
#   4: @flatten_163:CNode_224{[0]: ValueNode<Primitive> Switch, [1]: CNode_223, [2]: ValueNode<FuncGraph> flatten_225, [3]: ValueNode<FuncGraph> flatten_226}
#   5: @flatten_163:CNode_227{[0]: CNode_224}
#   6: @flatten_163:CNode_228{[0]: ValueNode<Primitive> Return, [1]: CNode_227}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_171 : 00000216313DE950
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_171 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_104]() {
  %1(CNode_229) = getattr(%para49_x, "unsqueeze")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:571/            x = x.unsqueeze(0)/
  %2(x) = %1(I64(0))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:571/            x = x.unsqueeze(0)/
  %3(CNode_231) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_230(%2, Bool(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file E:\mindspore\mydemo.py:42/        x = self.max_pool2d(self.relu(self.conv2(x)))/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:571/            x = x.unsqueeze(0)/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_171:CNode_229{[0]: ValueNode<Primitive> getattr, [1]: param_x, [2]: ValueNode<StringImm> unsqueeze}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_171:x{[0]: CNode_229, [1]: ValueNode<Int64Imm> 0}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_171:CNode_232{[0]: ValueNode<Primitive> Return, [1]: CNode_231}
#   4: @mindspore_nn_layer_pooling_MaxPool2d_construct_171:CNode_231{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_230, [1]: x, [2]: ValueNode<BoolImm> true}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_172 : 00000216313DD410
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_172 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_104]() {
  %1(CNode_233) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_230(%para49_x, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file E:\mindspore\mydemo.py:42/        x = self.max_pool2d(self.relu(self.conv2(x)))/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:570/        if x.ndim == 3:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_172:CNode_234{[0]: ValueNode<Primitive> Return, [1]: CNode_233}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_172:CNode_233{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_230, [1]: param_x, [2]: ValueNode<BoolImm> false}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_175 : 00000216313D9450
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_175 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_105]() {
  %1(CNode_236) = call @mindspore_nn_layer_conv_Conv2d_construct_235()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/conv2-Conv2d)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/conv2-Conv2d)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_175:CNode_236{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_235}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_175:CNode_237{[0]: ValueNode<Primitive> Return, [1]: CNode_236}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_178 : 00000216313DB430
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_178 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_100]() {
  %1(CNode_239) = call @mindspore_nn_layer_conv_Conv2d_construct_238()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/conv1-Conv2d)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/conv1-Conv2d)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:362/        if self.has_bias:/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_178:CNode_239{[0]: ValueNode<FuncGraph> mindspore_nn_layer_conv_Conv2d_construct_238}
#   2: @mindspore_nn_layer_conv_Conv2d_construct_178:CNode_240{[0]: ValueNode<Primitive> Return, [1]: CNode_239}


subgraph attr:
skip_auto_parallel_compile : 1
training : 1
subgraph instance: mindspore_nn_optim_momentum_Momentum_construct_186 : 00000216313D6480
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\optim\momentum.py:215/    def construct(self, gradients):/
subgraph @mindspore_nn_optim_momentum_Momentum_construct_186 parent: [subgraph @mindspore_nn_optim_momentum_Momentum_construct_96]() {
  %1(lr) = $(mindspore_nn_optim_momentum_Momentum_construct_65):call @get_lr_75()
      #scope: (Default/optimizer-Momentum)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\optim\momentum.py:222/        lr = self.get_lr()/
  %2(CNode_132) = $(mindspore_nn_optim_momentum_Momentum_construct_96):S_Prim_Partial[side_effect_propagate: I64(1)](S_Prim_momentum_opt, S_Prim_ApplyMomentum[use_nesterov: Bool(0), use_locking: Bool(0), gradient_scale: F32(1), input_names: ["variable", "accumulation", "learning_rate", "gradient", "momentum"], output_names: ["output"], side_effect_mem: Bool(1)], %para20_momentum, %1)
      : (<null>, <null>, <Ref[Tensor[Float32]], (), ref_key=:momentum>, <null>) -> (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\optim\momentum.py:238/                success = self.hyper_map_reverse(F.partial(_momentum_opt, self.opt, self.momentum, lr),/
  %3(gradients) = $(mindspore_nn_optim_momentum_Momentum_construct_65):call @flatten_gradients_71(%para33_gradients)
      : (<null>) -> (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\optim\momentum.py:218/        gradients = self.flatten_gradients(gradients)/
  %4(gradients) = $(mindspore_nn_optim_momentum_Momentum_construct_65):call @decay_weight_72(%3)
      : (<null>) -> (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\optim\momentum.py:219/        gradients = self.decay_weight(gradients)/
  %5(gradients) = $(mindspore_nn_optim_momentum_Momentum_construct_65):call @gradients_centralization_73(%4)
      : (<null>) -> (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\optim\momentum.py:220/        gradients = self.gradients_centralization(gradients)/
  %6(gradients) = $(mindspore_nn_optim_momentum_Momentum_construct_65):call @scale_grad_74(%5)
      : (<null>) -> (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\optim\momentum.py:221/        gradients = self.scale_grad(gradients)/
  %7(CNode_133) = $(mindspore_nn_optim_momentum_Momentum_construct_65):MakeTuple(%para3_conv1.weight, %para4_conv2.weight, %para5_fc1.weight, %para6_fc1.bias, %para7_fc2.weight, %para8_fc2.bias, %para9_fc3.weight, %para10_fc3.bias)
      : (<Ref[Tensor[Float32]], (6, 1, 5, 5), ref_key=:conv1.weight>, <Ref[Tensor[Float32]], (16, 6, 5, 5), ref_key=:conv2.weight>, <Ref[Tensor[Float32]], (120, 784), ref_key=:fc1.weight>, <Ref[Tensor[Float32]], (120), ref_key=:fc1.bias>, <Ref[Tensor[Float32]], (84, 120), ref_key=:fc2.weight>, <Ref[Tensor[Float32]], (84), ref_key=:fc2.bias>, <Ref[Tensor[Float32]], (10, 84), ref_key=:fc3.weight>, <Ref[Tensor[Float32]], (10), ref_key=:fc3.bias>) -> (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\optim\momentum.py:216/        params = self.params/
  %8(CNode_134) = $(mindspore_nn_optim_momentum_Momentum_construct_65):MakeTuple(%para12_moments.conv1.weight, %para13_moments.conv2.weight, %para14_moments.fc1.weight, %para15_moments.fc1.bias, %para16_moments.fc2.weight, %para17_moments.fc2.bias, %para18_moments.fc3.weight, %para19_moments.fc3.bias)
      : (<Ref[Tensor[Float32]], (6, 1, 5, 5), ref_key=:moments.conv1.weight>, <Ref[Tensor[Float32]], (16, 6, 5, 5), ref_key=:moments.conv2.weight>, <Ref[Tensor[Float32]], (120, 784), ref_key=:moments.fc1.weight>, <Ref[Tensor[Float32]], (120), ref_key=:moments.fc1.bias>, <Ref[Tensor[Float32]], (84, 120), ref_key=:moments.fc2.weight>, <Ref[Tensor[Float32]], (84), ref_key=:moments.fc2.bias>, <Ref[Tensor[Float32]], (10, 84), ref_key=:moments.fc3.weight>, <Ref[Tensor[Float32]], (10), ref_key=:moments.fc3.bias>) -> (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\optim\momentum.py:217/        moments = self.moments/
  %9(success) = $(mindspore_nn_optim_momentum_Momentum_construct_96):S_Prim_hyper_map(%2, %6, %7, %8, (Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0)), (Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0), Bool(0)))
      : (<null>, <null>, <null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\optim\momentum.py:238/                success = self.hyper_map_reverse(F.partial(_momentum_opt, self.opt, self.momentum, lr),/
  Return(%9)
      : (<null>)
      #scope: (Default/optimizer-Momentum)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\optim\momentum.py:240/        return success/
}
# Order:
#   1: @mindspore_nn_optim_momentum_Momentum_construct_186:CNode_241{[0]: ValueNode<Primitive> Return, [1]: success}


subgraph attr:
training : 1
after_block : 1
subgraph instance: mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_189 : 0000021629DFD9C0
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:777/    def construct(self, logits, labels):/
subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_189 parent: [subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_79]() {
  %1(CNode_243) = call @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_242()
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:781/            if self.reduction == 'mean':/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:781/            if self.reduction == 'mean':/
}
# Order:
#   1: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_189:CNode_244{[0]: ValueNode<FuncGraph> shape_245, [1]: param_logits}
#   2: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_189:CNode_246{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: ValueNode<Int64Imm> 1}
#   3: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_189:CNode_247{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_244, [2]: CNode_246}
#   4: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_189:labels{[0]: ValueNode<DoSignaturePrimitive> S_Prim_OneHot, [1]: param_labels, [2]: CNode_247, [3]: ValueNode<Tensor> Tensor(shape=[], dtype=Float32, value=1), [4]: ValueNode<Tensor> Tensor(shape=[], dtype=Float32, value=0)}
#   5: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_189:CNode_243{[0]: ValueNode<FuncGraph> mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_242}
#   6: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_189:CNode_248{[0]: ValueNode<Primitive> Return, [1]: CNode_243}


subgraph attr:
after_block : 1
training : 1
subgraph instance: L_mindspore_nn_layer_basic_Dense_construct_201 : 0000021629DFA4A0
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_basic_Dense_construct_201 parent: [subgraph @L_mindspore_nn_layer_basic_Dense_construct_138](%para62_) {
  %1(CNode_250) = call @L_mindspore_nn_layer_basic_Dense_construct_249()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:628/        if self.has_bias:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:628/        if self.has_bias:/
}
# Order:
#   1: @L_mindspore_nn_layer_basic_Dense_construct_201:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MatMul, [1]: param_phi_x, [2]: param_L_fc3.weight}
#   2: @L_mindspore_nn_layer_basic_Dense_construct_201:CNode_250{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_basic_Dense_construct_249}
#   3: @L_mindspore_nn_layer_basic_Dense_construct_201:CNode_251{[0]: ValueNode<Primitive> Return, [1]: CNode_250}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_basic_Dense_construct_198 : 0000021629DFC9D0
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_basic_Dense_construct_198 parent: [subgraph @L_mindspore_nn_layer_basic_Dense_construct_138]() {
  %1(CNode_252) = S_Prim_negative(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:626/            x = self.reshape(x, (-1, x_shape[-1]))/
  %2(x_shape) = $(L_mindspore_nn_layer_basic_Dense_construct_138):S_Prim_Shape(%para52_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:623/        x_shape = self.shape_op(x)/
  %3(CNode_253) = S_Prim_negative(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:626/            x = self.reshape(x, (-1, x_shape[-1]))/
  %4(CNode_254) = S_Prim_getitem(%2, %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:626/            x = self.reshape(x, (-1, x_shape[-1]))/
  %5(CNode_255) = S_Prim_MakeTuple(%1, %4)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:626/            x = self.reshape(x, (-1, x_shape[-1]))/
  %6(x) = S_Prim_Reshape[input_names: ["tensor", "shape"], output_names: ["output"]](%para52_x, %5)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:626/            x = self.reshape(x, (-1, x_shape[-1]))/
  Return(%6)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:626/            x = self.reshape(x, (-1, x_shape[-1]))/
}
# Order:
#   1: @L_mindspore_nn_layer_basic_Dense_construct_198:CNode_252{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: ValueNode<Int64Imm> 1}
#   2: @L_mindspore_nn_layer_basic_Dense_construct_198:CNode_253{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: ValueNode<Int64Imm> 1}
#   3: @L_mindspore_nn_layer_basic_Dense_construct_198:CNode_254{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: x_shape, [2]: CNode_253}
#   4: @L_mindspore_nn_layer_basic_Dense_construct_198:CNode_255{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: CNode_252, [2]: CNode_254}
#   5: @L_mindspore_nn_layer_basic_Dense_construct_198:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Reshape, [1]: param_x, [2]: CNode_255}
#   6: @L_mindspore_nn_layer_basic_Dense_construct_198:CNode_256{[0]: ValueNode<Primitive> Return, [1]: x}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_basic_Dense_construct_199 : 0000021629DF9F50
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_basic_Dense_construct_199 parent: [subgraph @L_mindspore_nn_layer_basic_Dense_construct_138]() {
  Return(%para52_x)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:625/        if len(x_shape) != 2:/
}
# Order:
#   1: @L_mindspore_nn_layer_basic_Dense_construct_199:CNode_257{[0]: ValueNode<Primitive> Return, [1]: param_x}


subgraph attr:
training : 1
subgraph instance: check_axis_valid_213 : 00000216313DA990
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:457/    def check_axis_valid(self, axis, ndim):/
subgraph @check_axis_valid_213() {
  %1(CNode_258) = raise[side_effect_io: Bool(1)]("ValueError", "'start_dim' or 'end_dim' out of range.", "None")
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:459/            raise ValueError("'start_dim' or 'end_dim' out of range.")/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:459/            raise ValueError("'start_dim' or 'end_dim' out of range.")/
}
# Order:
#   1: @check_axis_valid_213:CNode_258{[0]: ValueNode<Primitive> raise, [1]: ValueNode<StringImm> ValueError, [2]: ValueNode<StringImm> 'start_dim' or 'end_dim' out of range., [3]: ValueNode<StringImm> None}
#   2: @check_axis_valid_213:CNode_259{[0]: ValueNode<Primitive> Return, [1]: CNode_258}


subgraph attr:
training : 1
subgraph instance: check_axis_valid_214 : 00000216313E13D0
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:457/    def check_axis_valid(self, axis, ndim):/
subgraph @check_axis_valid_214() {
  %1(CNode_261) = call @check_axis_valid_260()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:458/        if axis < -ndim or axis >= ndim:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:458/        if axis < -ndim or axis >= ndim:/
}
# Order:
#   1: @check_axis_valid_214:CNode_261{[0]: ValueNode<FuncGraph> check_axis_valid_260}
#   2: @check_axis_valid_214:CNode_262{[0]: ValueNode<Primitive> Return, [1]: CNode_261}


subgraph attr:
training : 1
subgraph instance: check_axis_valid_208 : 00000216313D8460
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:457/    def check_axis_valid(self, axis, ndim):/
subgraph @check_axis_valid_208 parent: [subgraph @check_axis_valid_153]() {
  %1(CNode_204) = $(check_axis_valid_153):S_Prim_negative(%para56_ndim)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:458/        if axis < -ndim or axis >= ndim:/
  %2(CNode_205) = $(check_axis_valid_153):S_Prim_less(%para55_axis, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:458/        if axis < -ndim or axis >= ndim:/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:458/        if axis < -ndim or axis >= ndim:/
}
# Order:
#   1: @check_axis_valid_208:CNode_263{[0]: ValueNode<Primitive> Return, [1]: CNode_205}


subgraph attr:
training : 1
subgraph instance: check_axis_valid_209 : 00000216313D7F10
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:457/    def check_axis_valid(self, axis, ndim):/
subgraph @check_axis_valid_209 parent: [subgraph @check_axis_valid_153]() {
  %1(CNode_264) = S_Prim_greater_equal(%para55_axis, %para56_ndim)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:458/        if axis < -ndim or axis >= ndim:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:458/        if axis < -ndim or axis >= ndim:/
}
# Order:
#   1: @check_axis_valid_209:CNode_264{[0]: ValueNode<DoSignaturePrimitive> S_Prim_greater_equal, [1]: param_axis, [2]: param_ndim}
#   2: @check_axis_valid_209:CNode_265{[0]: ValueNode<Primitive> Return, [1]: CNode_264}


subgraph attr:
subgraph instance: flatten_225 : 0000021629DF8A10
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_225() {
  %1(CNode_266) = JoinedStr("For 'flatten', argument 'input' must be Tensor.")
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1732/        raise TypeError(f"For 'flatten', argument 'input' must be Tensor.")/
  %2(CNode_267) = raise[side_effect_io: Bool(1)]("TypeError", %1, "None")
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1732/        raise TypeError(f"For 'flatten', argument 'input' must be Tensor.")/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1732/        raise TypeError(f"For 'flatten', argument 'input' must be Tensor.")/
}
# Order:
#   1: @flatten_225:CNode_266{[0]: ValueNode<Primitive> JoinedStr, [1]: ValueNode<StringImm> For 'flatten', argument 'input' must be Tensor.}
#   2: @flatten_225:CNode_267{[0]: ValueNode<Primitive> raise, [1]: ValueNode<StringImm> TypeError, [2]: CNode_266, [3]: ValueNode<StringImm> None}
#   3: @flatten_225:CNode_268{[0]: ValueNode<Primitive> Return, [1]: CNode_267}


subgraph attr:
subgraph instance: flatten_226 : 00000216313E03E0
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_226 parent: [subgraph @flatten_163]() {
  %1(CNode_270) = call @flatten_269()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1731/    if not isinstance(input, Tensor):/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1731/    if not isinstance(input, Tensor):/
}
# Order:
#   1: @flatten_226:CNode_270{[0]: ValueNode<FuncGraph> flatten_269}
#   2: @flatten_226:CNode_271{[0]: ValueNode<Primitive> Return, [1]: CNode_270}


subgraph attr:
training : 1
after_block : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_230 : 00000216313D7470
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_230(%para63_, %para64_) {
  %1(CNode_273) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_272()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:573/        if self.use_pad:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:573/        if self.use_pad:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_230:CNode_273{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_272}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_230:CNode_274{[0]: ValueNode<Primitive> Return, [1]: CNode_273}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_235 : 00000216313DC970
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_235 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_105]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_105):S_Prim_Conv2D[out_channel: I64(16), kernel_size: (I64(5), I64(5)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(1), stride: (I64(1), I64(1), I64(1), I64(1)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1), pad_list: (I64(2), I64(2), I64(2), I64(2))](%para50_x, %para4_conv2.weight)
      : (<null>, <Ref[Tensor[Float32]], (16, 6, 5, 5), ref_key=:conv2.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/conv2-Conv2d)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/conv2-Conv2d)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_235:CNode_275{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_conv_Conv2d_construct_238 : 00000216313DB980
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:360/    def construct(self, x):/
subgraph @mindspore_nn_layer_conv_Conv2d_construct_238 parent: [subgraph @mindspore_nn_layer_conv_Conv2d_construct_100]() {
  %1(output) = $(mindspore_nn_layer_conv_Conv2d_construct_100):S_Prim_Conv2D[out_channel: I64(6), kernel_size: (I64(5), I64(5)), group: I64(1), format: "NCHW", mode: I64(1), pad: (I64(0), I64(0), I64(0), I64(0)), pad_mode: I64(1), stride: (I64(1), I64(1), I64(1), I64(1)), dilation: (I64(1), I64(1), I64(1), I64(1)), input_names: ["x", "w"], output_names: ["output"], groups: I64(1), pad_list: (I64(2), I64(2), I64(2), I64(2))](%para51_x, %para3_conv1.weight)
      : (<null>, <Ref[Tensor[Float32]], (6, 1, 5, 5), ref_key=:conv1.weight>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/conv1-Conv2d)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:361/        output = self.conv2d(x, self.weight)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/conv1-Conv2d)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\conv.py:364/        return output/
}
# Order:
#   1: @mindspore_nn_layer_conv_Conv2d_construct_238:CNode_276{[0]: ValueNode<Primitive> Return, [1]: output}


subgraph attr:
subgraph instance: shape_245 : 0000021629DFE9B0
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1484/def shape(input_x):/
subgraph @shape_245(%para65_input_x) {
  %1(CNode_277) = S_Prim_Shape(%para65_input_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1510/    return shape_(input_x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1510/    return shape_(input_x)/
}
# Order:
#   1: @shape_245:CNode_277{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Shape, [1]: param_input_x}
#   2: @shape_245:CNode_278{[0]: ValueNode<Primitive> Return, [1]: CNode_277}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_242 : 0000021629DFE460
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:777/    def construct(self, logits, labels):/
subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_242 parent: [subgraph @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_189]() {
  %1(CNode_244) = $(mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_189):call @shape_245(%para41_logits)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:784/            labels = self.one_hot(labels, F.shape(logits)[-1], self.on_value, self.off_value)/
  %2(CNode_246) = $(mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_189):S_Prim_negative(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:784/            labels = self.one_hot(labels, F.shape(logits)[-1], self.on_value, self.off_value)/
  %3(CNode_247) = $(mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_189):S_Prim_getitem(%1, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:784/            labels = self.one_hot(labels, F.shape(logits)[-1], self.on_value, self.off_value)/
  %4(labels) = $(mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_189):S_Prim_OneHot[axis: I64(-1), input_names: ["indices", "depth", "on_value", "off_value"], output_names: ["output"]](%para42_labels, %3, Tensor(shape=[], dtype=Float32, value=1), Tensor(shape=[], dtype=Float32, value=0))
      : (<null>, <null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:784/            labels = self.one_hot(labels, F.shape(logits)[-1], self.on_value, self.off_value)/
  %5(CNode_279) = S_Prim_SoftmaxCrossEntropyWithLogits(%para41_logits, %4)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:785/        x = self.softmax_cross_entropy(logits, labels)[0]/
  %6(x) = S_Prim_getitem(%5, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:785/        x = self.softmax_cross_entropy(logits, labels)[0]/
  %7(CNode_281) = call @get_loss_280(%6)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:786/        return self.get_loss(x)/
  Return(%7)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:786/        return self.get_loss(x)/
}
# Order:
#   1: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_242:CNode_279{[0]: ValueNode<DoSignaturePrimitive> S_Prim_SoftmaxCrossEntropyWithLogits, [1]: param_logits, [2]: labels}
#   2: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_242:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_279, [2]: ValueNode<Int64Imm> 0}
#   3: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_242:CNode_281{[0]: ValueNode<FuncGraph> get_loss_280, [1]: x}
#   4: @mindspore_nn_loss_loss_SoftmaxCrossEntropyWithLogits_construct_242:CNode_282{[0]: ValueNode<Primitive> Return, [1]: CNode_281}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_basic_Dense_construct_249 : 0000021629DFA9F0
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_basic_Dense_construct_249 parent: [subgraph @L_mindspore_nn_layer_basic_Dense_construct_201]() {
  %1(CNode_284) = call @L_mindspore_nn_layer_basic_Dense_construct_283()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:629/            x = self.bias_add(x, self.bias)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:629/            x = self.bias_add(x, self.bias)/
}
# Order:
#   1: @L_mindspore_nn_layer_basic_Dense_construct_249:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_BiasAdd, [1]: x, [2]: param_L_fc3.bias}
#   2: @L_mindspore_nn_layer_basic_Dense_construct_249:CNode_284{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_basic_Dense_construct_283}
#   3: @L_mindspore_nn_layer_basic_Dense_construct_249:CNode_285{[0]: ValueNode<Primitive> Return, [1]: CNode_284}


subgraph attr:
training : 1
after_block : 1
subgraph instance: check_axis_valid_260 : 00000216313DF940
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:457/    def check_axis_valid(self, axis, ndim):/
subgraph @check_axis_valid_260() {
  Return(None)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:457/    def check_axis_valid(self, axis, ndim):/
}
# Order:
#   1: @check_axis_valid_260:CNode_286{[0]: ValueNode<Primitive> Return, [1]: ValueNode<None> None}


subgraph attr:
after_block : 1
subgraph instance: flatten_269 : 00000216313E1920
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_269 parent: [subgraph @flatten_163]() {
  %1(CNode_287) = S_Prim_isinstance(%para60_start_dim, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %2(CNode_288) = S_Prim_logical_not(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %3(CNode_289) = Cond(%2, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %4(CNode_290) = Switch(%3, @flatten_291, @flatten_292)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %5(CNode_293) = %4()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %6(CNode_294) = Cond(%5, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %7(CNode_295) = Switch(%6, @flatten_296, @flatten_297)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %8(CNode_298) = %7()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
}
# Order:
#   1: @flatten_269:CNode_287{[0]: ValueNode<DoSignaturePrimitive> S_Prim_isinstance, [1]: param_start_dim, [2]: ValueNode<ClassType> class 'int'}
#   2: @flatten_269:CNode_288{[0]: ValueNode<DoSignaturePrimitive> S_Prim_logical_not, [1]: CNode_287}
#   3: @flatten_269:CNode_289{[0]: ValueNode<Primitive> Cond, [1]: CNode_288, [2]: ValueNode<BoolImm> false}
#   4: @flatten_269:CNode_290{[0]: ValueNode<Primitive> Switch, [1]: CNode_289, [2]: ValueNode<FuncGraph> flatten_291, [3]: ValueNode<FuncGraph> flatten_292}
#   5: @flatten_269:CNode_293{[0]: CNode_290}
#   6: @flatten_269:CNode_294{[0]: ValueNode<Primitive> Cond, [1]: CNode_293, [2]: ValueNode<BoolImm> false}
#   7: @flatten_269:CNode_295{[0]: ValueNode<Primitive> Switch, [1]: CNode_294, [2]: ValueNode<FuncGraph> flatten_296, [3]: ValueNode<FuncGraph> flatten_297}
#   8: @flatten_269:CNode_298{[0]: CNode_295}
#   9: @flatten_269:CNode_299{[0]: ValueNode<Primitive> Return, [1]: CNode_298}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_272 : 00000216313DF3F0
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_272 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_230]() {
  %1(CNode_301) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_300()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:581/            out = self.max_pool(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:581/            out = self.max_pool(x)/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_272:out{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MaxPool, [1]: param_phi_x}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_272:CNode_301{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_300}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_272:CNode_302{[0]: ValueNode<Primitive> Return, [1]: CNode_301}


subgraph attr:
training : 1
subgraph instance: get_loss_280 : 0000021629E03EB0
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:125/    def get_loss(self, x, weights=1.0):/
subgraph @get_loss_280(%para66_x, %para67_weights) {
  %1(CNode_304) = call @get_loss_303()
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:143/        if self.reduce and self.average:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:143/        if self.reduce and self.average:/
}
# Order:
#   1: @get_loss_280:input_dtype{[0]: ValueNode<Primitive> getattr, [1]: param_x, [2]: ValueNode<StringImm> dtype}
#   2: @get_loss_280:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Cast, [1]: param_x, [2]: ValueNode<Float> Float32}
#   3: @get_loss_280:weights{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Cast, [1]: param_weights, [2]: ValueNode<Float> Float32}
#   4: @get_loss_280:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Mul, [1]: weights, [2]: x}
#   5: @get_loss_280:CNode_304{[0]: ValueNode<FuncGraph> get_loss_303}
#   6: @get_loss_280:CNode_305{[0]: ValueNode<Primitive> Return, [1]: CNode_304}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_basic_Dense_construct_283 : 0000021629DFAF40
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_basic_Dense_construct_283 parent: [subgraph @L_mindspore_nn_layer_basic_Dense_construct_249]() {
  %1(CNode_307) = call @L_mindspore_nn_layer_basic_Dense_construct_306()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:630/        if self.activation_flag:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:630/        if self.activation_flag:/
}
# Order:
#   1: @L_mindspore_nn_layer_basic_Dense_construct_283:CNode_307{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_basic_Dense_construct_306}
#   2: @L_mindspore_nn_layer_basic_Dense_construct_283:CNode_308{[0]: ValueNode<Primitive> Return, [1]: CNode_307}


subgraph attr:
subgraph instance: flatten_296 : 0000021629DF84C0
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_296() {
  %1(CNode_309) = JoinedStr("For 'flatten', both 'start_dim' and 'end_dim' must be int.")
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1735/        raise TypeError(f"For 'flatten', both 'start_dim' and 'end_dim' must be int.")/
  %2(CNode_310) = raise[side_effect_io: Bool(1)]("TypeError", %1, "None")
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1735/        raise TypeError(f"For 'flatten', both 'start_dim' and 'end_dim' must be int.")/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1735/        raise TypeError(f"For 'flatten', both 'start_dim' and 'end_dim' must be int.")/
}
# Order:
#   1: @flatten_296:CNode_309{[0]: ValueNode<Primitive> JoinedStr, [1]: ValueNode<StringImm> For 'flatten', both 'start_dim' and 'end_dim' must be int.}
#   2: @flatten_296:CNode_310{[0]: ValueNode<Primitive> raise, [1]: ValueNode<StringImm> TypeError, [2]: CNode_309, [3]: ValueNode<StringImm> None}
#   3: @flatten_296:CNode_311{[0]: ValueNode<Primitive> Return, [1]: CNode_310}


subgraph attr:
subgraph instance: flatten_297 : 0000021629DEC580
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_297 parent: [subgraph @flatten_163]() {
  %1(CNode_313) = call @flatten_312()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
}
# Order:
#   1: @flatten_297:CNode_313{[0]: ValueNode<FuncGraph> flatten_312}
#   2: @flatten_297:CNode_314{[0]: ValueNode<Primitive> Return, [1]: CNode_313}


subgraph attr:
subgraph instance: flatten_291 : 0000021629DEA050
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_291 parent: [subgraph @flatten_269]() {
  %1(CNode_287) = $(flatten_269):S_Prim_isinstance(%para60_start_dim, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %2(CNode_288) = $(flatten_269):S_Prim_logical_not(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
}
# Order:
#   1: @flatten_291:CNode_315{[0]: ValueNode<Primitive> Return, [1]: CNode_288}


subgraph attr:
subgraph instance: flatten_292 : 00000216313E0930
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_292 parent: [subgraph @flatten_163]() {
  %1(CNode_316) = S_Prim_isinstance(%para61_end_dim, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %2(CNode_317) = S_Prim_logical_not(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %3(CNode_318) = Cond(%2, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1734/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
  %4(CNode_319) = Switch(%3, @flatten_320, @flatten_321)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1734/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
  %5(CNode_322) = %4()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1734/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
  Return(%5)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
}
# Order:
#   1: @flatten_292:CNode_316{[0]: ValueNode<DoSignaturePrimitive> S_Prim_isinstance, [1]: param_end_dim, [2]: ValueNode<ClassType> class 'int'}
#   2: @flatten_292:CNode_317{[0]: ValueNode<DoSignaturePrimitive> S_Prim_logical_not, [1]: CNode_316}
#   3: @flatten_292:CNode_318{[0]: ValueNode<Primitive> Cond, [1]: CNode_317, [2]: ValueNode<BoolImm> false}
#   4: @flatten_292:CNode_319{[0]: ValueNode<Primitive> Switch, [1]: CNode_318, [2]: ValueNode<FuncGraph> flatten_320, [3]: ValueNode<FuncGraph> flatten_321}
#   5: @flatten_292:CNode_322{[0]: CNode_319}
#   6: @flatten_292:CNode_323{[0]: ValueNode<Primitive> Return, [1]: CNode_322}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_300 : 00000216313DBED0
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_300 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_272]() {
  %1(CNode_324) = Cond(%para64_phi_expand_batch, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:582/        if expand_batch:/
  %2(CNode_325) = Switch(%1, @mindspore_nn_layer_pooling_MaxPool2d_construct_326, @mindspore_nn_layer_pooling_MaxPool2d_construct_327)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:582/        if expand_batch:/
  %3(CNode_328) = %2()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:582/        if expand_batch:/
  %4(CNode_330) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_329(%3)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file E:\mindspore\mydemo.py:42/        x = self.max_pool2d(self.relu(self.conv2(x)))/
  Return(%4)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:582/        if expand_batch:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_300:CNode_324{[0]: ValueNode<Primitive> Cond, [1]: param_phi_expand_batch, [2]: ValueNode<BoolImm> false}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_300:CNode_325{[0]: ValueNode<Primitive> Switch, [1]: CNode_324, [2]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_326, [3]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_327}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_300:CNode_328{[0]: CNode_325}
#   4: @mindspore_nn_layer_pooling_MaxPool2d_construct_300:CNode_330{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_329, [1]: CNode_328}
#   5: @mindspore_nn_layer_pooling_MaxPool2d_construct_300:CNode_331{[0]: ValueNode<Primitive> Return, [1]: CNode_330}


subgraph attr:
training : 1
subgraph instance: get_loss_303 : 0000021629E01980
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:125/    def get_loss(self, x, weights=1.0):/
subgraph @get_loss_303 parent: [subgraph @get_loss_280]() {
  %1(CNode_333) = call @get_loss_332()
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:144/            x = self.reduce_mean(x, self.get_axis(x))/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:144/            x = self.reduce_mean(x, self.get_axis(x))/
}
# Order:
#   1: @get_loss_303:CNode_334{[0]: ValueNode<FuncGraph> get_axis_335, [1]: x}
#   2: @get_loss_303:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_ReduceMean, [1]: x, [2]: CNode_334}
#   3: @get_loss_303:CNode_333{[0]: ValueNode<FuncGraph> get_loss_332}
#   4: @get_loss_303:CNode_336{[0]: ValueNode<Primitive> Return, [1]: CNode_333}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_basic_Dense_construct_306 : 0000021629DFB490
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_basic_Dense_construct_306 parent: [subgraph @L_mindspore_nn_layer_basic_Dense_construct_249]() {
  %1(CNode_338) = call @L_mindspore_nn_layer_basic_Dense_construct_337()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:630/        if self.activation_flag:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:630/        if self.activation_flag:/
}
# Order:
#   1: @L_mindspore_nn_layer_basic_Dense_construct_306:CNode_338{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_basic_Dense_construct_337}
#   2: @L_mindspore_nn_layer_basic_Dense_construct_306:CNode_339{[0]: ValueNode<Primitive> Return, [1]: CNode_338}


subgraph attr:
after_block : 1
subgraph instance: flatten_312 : 0000021629DECAD0
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_312 parent: [subgraph @flatten_163]() {
  %1(CNode_340) = S_Prim_check_flatten_order[constexpr_prim: Bool(1)](%para59_order)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1736/    check_flatten_order_const(order)/
  %2(CNode_341) = StopGradient(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
  %3(CNode_342) = S_Prim_equal(%para59_order, "F")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1737/    if order == 'F':/
  %4(CNode_343) = Cond(%3, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1737/    if order == 'F':/
  %5(CNode_344) = Switch(%4, @flatten_345, @flatten_346)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1737/    if order == 'F':/
  %6(CNode_347) = %5()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1737/    if order == 'F':/
  %7(CNode_348) = Depend[side_effect_propagate: I64(1)](%6, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1737/    if order == 'F':/
  Return(%7)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1737/    if order == 'F':/
}
# Order:
#   1: @flatten_312:CNode_340{[0]: ValueNode<DoSignaturePrimitive> S_Prim_check_flatten_order, [1]: param_order}
#   2: @flatten_312:CNode_342{[0]: ValueNode<DoSignaturePrimitive> S_Prim_equal, [1]: param_order, [2]: ValueNode<StringImm> F}
#   3: @flatten_312:CNode_343{[0]: ValueNode<Primitive> Cond, [1]: CNode_342, [2]: ValueNode<BoolImm> false}
#   4: @flatten_312:CNode_344{[0]: ValueNode<Primitive> Switch, [1]: CNode_343, [2]: ValueNode<FuncGraph> flatten_345, [3]: ValueNode<FuncGraph> flatten_346}
#   5: @flatten_312:CNode_347{[0]: CNode_344}
#   6: @flatten_312:CNode_349{[0]: ValueNode<Primitive> Return, [1]: CNode_348}


subgraph attr:
subgraph instance: flatten_320 : 0000021629DEF000
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_320 parent: [subgraph @flatten_292]() {
  %1(CNode_316) = $(flatten_292):S_Prim_isinstance(%para61_end_dim, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  %2(CNode_317) = $(flatten_292):S_Prim_logical_not(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1734/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
}
# Order:
#   1: @flatten_320:CNode_350{[0]: ValueNode<Primitive> Return, [1]: CNode_317}


subgraph attr:
subgraph instance: flatten_321 : 00000216313E0E80
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_321 parent: [subgraph @flatten_163]() {
  %1(CNode_351) = S_Prim_isinstance(%para60_start_dim, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1734/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
  %2(CNode_352) = Cond(%1, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1734/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
  %3(CNode_353) = Switch(%2, @flatten_354, @flatten_355)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1734/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
  %4(CNode_356) = %3()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1734/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
  Return(%4)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
}
# Order:
#   1: @flatten_321:CNode_351{[0]: ValueNode<DoSignaturePrimitive> S_Prim_isinstance, [1]: param_start_dim, [2]: ValueNode<ClassType> class 'bool'}
#   2: @flatten_321:CNode_352{[0]: ValueNode<Primitive> Cond, [1]: CNode_351, [2]: ValueNode<BoolImm> false}
#   3: @flatten_321:CNode_353{[0]: ValueNode<Primitive> Switch, [1]: CNode_352, [2]: ValueNode<FuncGraph> flatten_354, [3]: ValueNode<FuncGraph> flatten_355}
#   4: @flatten_321:CNode_356{[0]: CNode_353}
#   5: @flatten_321:CNode_357{[0]: ValueNode<Primitive> Return, [1]: CNode_356}


subgraph attr:
training : 1
after_block : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_329 : 00000216313DE400
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_329(%para68_) {
  %1(CNode_359) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_358()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:587/        if self.use_pad and not self.return_indices:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:587/        if self.use_pad and not self.return_indices:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_329:CNode_359{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_358}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_329:CNode_360{[0]: ValueNode<Primitive> Return, [1]: CNode_359}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_326 : 00000216313D99A0
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_326 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_272]() {
  %1(out) = $(mindspore_nn_layer_pooling_MaxPool2d_construct_272):S_Prim_MaxPool[kernel_size: (I64(1), I64(1), I64(2), I64(2)), input_names: ["x"], strides: (I64(1), I64(1), I64(2), I64(2)), pad_mode: I64(2), format: "NCHW", output_names: ["output"]](%para63_phi_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:581/            out = self.max_pool(x)/
  %2(CNode_361) = S_Prim_isinstance(%1, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:583/            if isinstance(out, tuple):/
  %3(CNode_362) = Cond(%2, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:583/            if isinstance(out, tuple):/
  %4(CNode_363) = Switch(%3, @mindspore_nn_layer_pooling_MaxPool2d_construct_364, @mindspore_nn_layer_pooling_MaxPool2d_construct_365)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:583/            if isinstance(out, tuple):/
  %5(CNode_366) = %4()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:583/            if isinstance(out, tuple):/
  %6(CNode_368) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_367(%5)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file E:\mindspore\mydemo.py:42/        x = self.max_pool2d(self.relu(self.conv2(x)))/
  Return(%6)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:583/            if isinstance(out, tuple):/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_326:CNode_361{[0]: ValueNode<DoSignaturePrimitive> S_Prim_isinstance, [1]: out, [2]: ValueNode<ClassType> class 'tuple'}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_326:CNode_362{[0]: ValueNode<Primitive> Cond, [1]: CNode_361, [2]: ValueNode<BoolImm> false}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_326:CNode_363{[0]: ValueNode<Primitive> Switch, [1]: CNode_362, [2]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_364, [3]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_365}
#   4: @mindspore_nn_layer_pooling_MaxPool2d_construct_326:CNode_366{[0]: CNode_363}
#   5: @mindspore_nn_layer_pooling_MaxPool2d_construct_326:CNode_368{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_367, [1]: CNode_366}
#   6: @mindspore_nn_layer_pooling_MaxPool2d_construct_326:CNode_369{[0]: ValueNode<Primitive> Return, [1]: CNode_368}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_327 : 00000216313DAEE0
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_327 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_272]() {
  %1(out) = $(mindspore_nn_layer_pooling_MaxPool2d_construct_272):S_Prim_MaxPool[kernel_size: (I64(1), I64(1), I64(2), I64(2)), input_names: ["x"], strides: (I64(1), I64(1), I64(2), I64(2)), pad_mode: I64(2), format: "NCHW", output_names: ["output"]](%para63_phi_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:581/            out = self.max_pool(x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:582/        if expand_batch:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_327:CNode_370{[0]: ValueNode<Primitive> Return, [1]: out}


subgraph attr:
training : 1
subgraph instance: get_axis_335 : 0000021629E00440
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:113/    def get_axis(self, x):/
subgraph @get_axis_335(%para69_x) {
  %1(shape) = call @shape_245(%para69_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:120/        shape = F.shape(x)/
  %2(length) = S_Prim_sequence_len(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:121/        length = F.tuple_len(shape)/
  %3(perm) = S_Prim_make_range(I64(0), %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:122/        perm = F.make_range(0, length)/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:123/        return perm/
}
# Order:
#   1: @get_axis_335:shape{[0]: ValueNode<FuncGraph> shape_245, [1]: param_x}
#   2: @get_axis_335:length{[0]: ValueNode<DoSignaturePrimitive> S_Prim_sequence_len, [1]: shape}
#   3: @get_axis_335:perm{[0]: ValueNode<DoSignaturePrimitive> S_Prim_make_range, [1]: ValueNode<Int64Imm> 0, [2]: length}
#   4: @get_axis_335:CNode_371{[0]: ValueNode<Primitive> Return, [1]: perm}


subgraph attr:
training : 1
subgraph instance: get_loss_332 : 0000021629E04400
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:125/    def get_loss(self, x, weights=1.0):/
subgraph @get_loss_332 parent: [subgraph @get_loss_303]() {
  %1(CNode_373) = call @get_loss_372()
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:145/        if self.reduce and not self.average:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:145/        if self.reduce and not self.average:/
}
# Order:
#   1: @get_loss_332:CNode_373{[0]: ValueNode<FuncGraph> get_loss_372}
#   2: @get_loss_332:CNode_374{[0]: ValueNode<Primitive> Return, [1]: CNode_373}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_basic_Dense_construct_337 : 0000021629DFB9E0
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_basic_Dense_construct_337 parent: [subgraph @L_mindspore_nn_layer_basic_Dense_construct_249]() {
  %1(x_shape) = $(L_mindspore_nn_layer_basic_Dense_construct_138):S_Prim_Shape(%para52_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:623/        x_shape = self.shape_op(x)/
  %2(CNode_375) = S_Prim_inner_len(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:632/        if len(x_shape) != 2:/
  %3(CNode_376) = S_Prim_not_equal(%2, I64(2))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:632/        if len(x_shape) != 2:/
  %4(CNode_377) = Cond(%3, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:632/        if len(x_shape) != 2:/
  %5(CNode_378) = Switch(%4, @L_mindspore_nn_layer_basic_Dense_construct_379, @L_mindspore_nn_layer_basic_Dense_construct_380)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:632/        if len(x_shape) != 2:/
  %6(CNode_381) = %5()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:632/        if len(x_shape) != 2:/
  %7(CNode_383) = call @L_mindspore_nn_layer_basic_Dense_construct_382(%6)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5)
      # In file E:\mindspore\mydemo.py:46/        x = self.fc3(x)/
  Return(%7)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:632/        if len(x_shape) != 2:/
}
# Order:
#   1: @L_mindspore_nn_layer_basic_Dense_construct_337:CNode_375{[0]: ValueNode<DoSignaturePrimitive> S_Prim_inner_len, [1]: x_shape}
#   2: @L_mindspore_nn_layer_basic_Dense_construct_337:CNode_376{[0]: ValueNode<DoSignaturePrimitive> S_Prim_not_equal, [1]: CNode_375, [2]: ValueNode<Int64Imm> 2}
#   3: @L_mindspore_nn_layer_basic_Dense_construct_337:CNode_377{[0]: ValueNode<Primitive> Cond, [1]: CNode_376, [2]: ValueNode<BoolImm> false}
#   4: @L_mindspore_nn_layer_basic_Dense_construct_337:CNode_378{[0]: ValueNode<Primitive> Switch, [1]: CNode_377, [2]: ValueNode<FuncGraph> L_mindspore_nn_layer_basic_Dense_construct_379, [3]: ValueNode<FuncGraph> L_mindspore_nn_layer_basic_Dense_construct_380}
#   5: @L_mindspore_nn_layer_basic_Dense_construct_337:CNode_381{[0]: CNode_378}
#   6: @L_mindspore_nn_layer_basic_Dense_construct_337:CNode_383{[0]: ValueNode<FuncGraph> L_mindspore_nn_layer_basic_Dense_construct_382, [1]: CNode_381}
#   7: @L_mindspore_nn_layer_basic_Dense_construct_337:CNode_384{[0]: ValueNode<Primitive> Return, [1]: CNode_383}


subgraph attr:
subgraph instance: flatten_345 : 0000021629DF5F90
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_345 parent: [subgraph @flatten_163]() {
  %1(x_rank) = S_Prim_Rank(%para58_input)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1738/        x_rank = rank_(input)/
  %2(CNode_385) = S_Prim_MakeTuple(I64(0), I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1740/        if x_rank in (0, 1):/
  %3(CNode_386) = S_Prim_in(%1, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1740/        if x_rank in (0, 1):/
  %4(CNode_387) = Cond(%3, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1740/        if x_rank in (0, 1):/
  %5(CNode_388) = Switch(%4, @flatten_389, @flatten_390)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1740/        if x_rank in (0, 1):/
  %6(CNode_391) = %5()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1740/        if x_rank in (0, 1):/
  Return(%6)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1740/        if x_rank in (0, 1):/
}
# Order:
#   1: @flatten_345:x_rank{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Rank, [1]: param_input}
#   2: @flatten_345:CNode_385{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: ValueNode<Int64Imm> 0, [2]: ValueNode<Int64Imm> 1}
#   3: @flatten_345:CNode_386{[0]: ValueNode<DoSignaturePrimitive> S_Prim_in, [1]: x_rank, [2]: CNode_385}
#   4: @flatten_345:CNode_387{[0]: ValueNode<Primitive> Cond, [1]: CNode_386, [2]: ValueNode<BoolImm> false}
#   5: @flatten_345:CNode_388{[0]: ValueNode<Primitive> Switch, [1]: CNode_387, [2]: ValueNode<FuncGraph> flatten_389, [3]: ValueNode<FuncGraph> flatten_390}
#   6: @flatten_345:CNode_391{[0]: CNode_388}
#   7: @flatten_345:CNode_392{[0]: ValueNode<Primitive> Return, [1]: CNode_391}


subgraph attr:
subgraph instance: flatten_346 : 0000021629DE9060
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_346 parent: [subgraph @flatten_163]() {
  %1(CNode_394) = call @flatten_393(%para58_input)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1737/    if order == 'F':/
}
# Order:
#   1: @flatten_346:CNode_395{[0]: ValueNode<Primitive> Return, [1]: CNode_394}
#   2: @flatten_346:CNode_394{[0]: ValueNode<FuncGraph> flatten_393, [1]: param_input}


subgraph attr:
subgraph instance: flatten_354 : 0000021629DEB590
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_354 parent: [subgraph @flatten_321]() {
  %1(CNode_351) = $(flatten_321):S_Prim_isinstance(%para60_start_dim, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1734/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1734/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
}
# Order:
#   1: @flatten_354:CNode_396{[0]: ValueNode<Primitive> Return, [1]: CNode_351}


subgraph attr:
subgraph instance: flatten_355 : 0000021629DEC030
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_355 parent: [subgraph @flatten_163]() {
  %1(CNode_397) = S_Prim_isinstance(%para61_end_dim, ClassType)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1734/            isinstance(start_dim, bool) or isinstance(end_dim, bool):/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1733/    if not isinstance(start_dim, int) or not isinstance(end_dim, int) or \/
}
# Order:
#   1: @flatten_355:CNode_397{[0]: ValueNode<DoSignaturePrimitive> S_Prim_isinstance, [1]: param_end_dim, [2]: ValueNode<ClassType> class 'bool'}
#   2: @flatten_355:CNode_398{[0]: ValueNode<Primitive> Return, [1]: CNode_397}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_358 : 00000216313DA440
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_358 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_329]() {
  %1(CNode_400) = call @mindspore_nn_layer_pooling_MaxPool2d_construct_399()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:587/        if self.use_pad and not self.return_indices:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:587/        if self.use_pad and not self.return_indices:/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_358:CNode_400{[0]: ValueNode<FuncGraph> mindspore_nn_layer_pooling_MaxPool2d_construct_399}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_358:CNode_401{[0]: ValueNode<Primitive> Return, [1]: CNode_400}


subgraph attr:
training : 1
after_block : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_367 : 00000216313DDEB0
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_367(%para70_) {
  Return(%para70_phi_out)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:583/            if isinstance(out, tuple):/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_367:CNode_402{[0]: ValueNode<Primitive> Return, [1]: param_phi_out}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_364 : 00000216313D9EF0
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_364 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_272]() {
  %1(out) = $(mindspore_nn_layer_pooling_MaxPool2d_construct_272):S_Prim_MaxPool[kernel_size: (I64(1), I64(1), I64(2), I64(2)), input_names: ["x"], strides: (I64(1), I64(1), I64(2), I64(2)), pad_mode: I64(2), format: "NCHW", output_names: ["output"]](%para63_phi_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:581/            out = self.max_pool(x)/
  %2(CNode_403) = S_Prim_getitem(%1, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %3(CNode_404) = getattr(%2, "squeeze")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %4(CNode_405) = %3(I64(0))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %5(CNode_406) = S_Prim_getitem(%1, I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %6(CNode_407) = getattr(%5, "squeeze")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %7(CNode_408) = %6(I64(0))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  %8(out) = S_Prim_MakeTuple(%4, %7)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:584/                out = (out[0].squeeze(0), out[1].squeeze(0))/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_364:CNode_403{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: out, [2]: ValueNode<Int64Imm> 0}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_364:CNode_404{[0]: ValueNode<Primitive> getattr, [1]: CNode_403, [2]: ValueNode<StringImm> squeeze}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_364:CNode_405{[0]: CNode_404, [1]: ValueNode<Int64Imm> 0}
#   4: @mindspore_nn_layer_pooling_MaxPool2d_construct_364:CNode_406{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: out, [2]: ValueNode<Int64Imm> 1}
#   5: @mindspore_nn_layer_pooling_MaxPool2d_construct_364:CNode_407{[0]: ValueNode<Primitive> getattr, [1]: CNode_406, [2]: ValueNode<StringImm> squeeze}
#   6: @mindspore_nn_layer_pooling_MaxPool2d_construct_364:CNode_408{[0]: CNode_407, [1]: ValueNode<Int64Imm> 0}
#   7: @mindspore_nn_layer_pooling_MaxPool2d_construct_364:out{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: CNode_405, [2]: CNode_408}
#   8: @mindspore_nn_layer_pooling_MaxPool2d_construct_364:CNode_409{[0]: ValueNode<Primitive> Return, [1]: out}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_365 : 00000216313DD960
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_365 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_272]() {
  %1(out) = $(mindspore_nn_layer_pooling_MaxPool2d_construct_272):S_Prim_MaxPool[kernel_size: (I64(1), I64(1), I64(2), I64(2)), input_names: ["x"], strides: (I64(1), I64(1), I64(2), I64(2)), pad_mode: I64(2), format: "NCHW", output_names: ["output"]](%para63_phi_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:581/            out = self.max_pool(x)/
  %2(CNode_410) = getattr(%1, "squeeze")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:586/                out = out.squeeze(0)/
  %3(out) = %2(I64(0))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:586/                out = out.squeeze(0)/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:586/                out = out.squeeze(0)/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_365:CNode_410{[0]: ValueNode<Primitive> getattr, [1]: out, [2]: ValueNode<StringImm> squeeze}
#   2: @mindspore_nn_layer_pooling_MaxPool2d_construct_365:out{[0]: CNode_410, [1]: ValueNode<Int64Imm> 0}
#   3: @mindspore_nn_layer_pooling_MaxPool2d_construct_365:CNode_411{[0]: ValueNode<Primitive> Return, [1]: out}


subgraph attr:
training : 1
subgraph instance: get_loss_372 : 0000021629E01430
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:125/    def get_loss(self, x, weights=1.0):/
subgraph @get_loss_372 parent: [subgraph @get_loss_303]() {
  %1(CNode_413) = call @get_loss_412()
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:145/        if self.reduce and not self.average:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:145/        if self.reduce and not self.average:/
}
# Order:
#   1: @get_loss_372:CNode_413{[0]: ValueNode<FuncGraph> get_loss_412}
#   2: @get_loss_372:CNode_414{[0]: ValueNode<Primitive> Return, [1]: CNode_413}


subgraph attr:
after_block : 1
training : 1
subgraph instance: L_mindspore_nn_layer_basic_Dense_construct_382 : 0000021629E03410
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_basic_Dense_construct_382(%para71_) {
  Return(%para71_phi_x)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:635/        return x/
}
# Order:
#   1: @L_mindspore_nn_layer_basic_Dense_construct_382:CNode_415{[0]: ValueNode<Primitive> Return, [1]: param_phi_x}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_basic_Dense_construct_379 : 0000021629DFC480
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_basic_Dense_construct_379 parent: [subgraph @L_mindspore_nn_layer_basic_Dense_construct_249]() {
  %1(x) = $(L_mindspore_nn_layer_basic_Dense_construct_201):S_Prim_MatMul[transpose_a: Bool(0), input_names: ["x1", "x2"], transpose_b: Bool(1), output_names: ["output"], transpose_x1: Bool(0), transpose_x2: Bool(1)](%para62_phi_x, %para54_L_fc3.weight)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:627/        x = self.matmul(x, self.weight)/
  %2(x) = $(L_mindspore_nn_layer_basic_Dense_construct_249):S_Prim_BiasAdd[format: "NCHW", input_names: ["x", "b"], output_names: ["output"], data_format: "NCHW"](%1, %para53_L_fc3.bias)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:629/            x = self.bias_add(x, self.bias)/
  %3(x_shape) = $(L_mindspore_nn_layer_basic_Dense_construct_138):S_Prim_Shape(%para52_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:623/        x_shape = self.shape_op(x)/
  %4(CNode_416) = S_Prim_negative(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
  %5(CNode_417) = S_Prim_make_slice(None, %4, None)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
  %6(CNode_418) = S_Prim_getitem(%3, %5)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
  %7(CNode_420) = call @L_shape_419(%2)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
  %8(CNode_421) = S_Prim_negative(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
  %9(CNode_422) = S_Prim_getitem(%7, %8)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
  %10(CNode_423) = S_Prim_MakeTuple(%9)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
  %11(out_shape) = S_Prim_add(%6, %10)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
  %12(x) = S_Prim_Reshape[input_names: ["tensor", "shape"], output_names: ["output"]](%2, %11)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:634/            x = self.reshape(x, out_shape)/
  Return(%12)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:633/            out_shape = x_shape[:-1] + (F.shape(x)[-1],)/
}
# Order:
#   1: @L_mindspore_nn_layer_basic_Dense_construct_379:CNode_416{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: ValueNode<Int64Imm> 1}
#   2: @L_mindspore_nn_layer_basic_Dense_construct_379:CNode_417{[0]: ValueNode<DoSignaturePrimitive> S_Prim_make_slice, [1]: ValueNode<None> None, [2]: CNode_416, [3]: ValueNode<None> None}
#   3: @L_mindspore_nn_layer_basic_Dense_construct_379:CNode_418{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: x_shape, [2]: CNode_417}
#   4: @L_mindspore_nn_layer_basic_Dense_construct_379:CNode_420{[0]: ValueNode<FuncGraph> L_shape_419, [1]: x}
#   5: @L_mindspore_nn_layer_basic_Dense_construct_379:CNode_421{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: ValueNode<Int64Imm> 1}
#   6: @L_mindspore_nn_layer_basic_Dense_construct_379:CNode_422{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: CNode_420, [2]: CNode_421}
#   7: @L_mindspore_nn_layer_basic_Dense_construct_379:CNode_423{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: CNode_422}
#   8: @L_mindspore_nn_layer_basic_Dense_construct_379:out_shape{[0]: ValueNode<DoSignaturePrimitive> S_Prim_add, [1]: CNode_418, [2]: CNode_423}
#   9: @L_mindspore_nn_layer_basic_Dense_construct_379:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Reshape, [1]: x, [2]: out_shape}
#  10: @L_mindspore_nn_layer_basic_Dense_construct_379:CNode_424{[0]: ValueNode<Primitive> Return, [1]: x}


subgraph attr:
training : 1
subgraph instance: L_mindspore_nn_layer_basic_Dense_construct_380 : 0000021629DFBF30
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:622/    def construct(self, x):/
subgraph @L_mindspore_nn_layer_basic_Dense_construct_380 parent: [subgraph @L_mindspore_nn_layer_basic_Dense_construct_249]() {
  %1(x) = $(L_mindspore_nn_layer_basic_Dense_construct_201):S_Prim_MatMul[transpose_a: Bool(0), input_names: ["x1", "x2"], transpose_b: Bool(1), output_names: ["output"], transpose_x1: Bool(0), transpose_x2: Bool(1)](%para62_phi_x, %para54_L_fc3.weight)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:627/        x = self.matmul(x, self.weight)/
  %2(x) = $(L_mindspore_nn_layer_basic_Dense_construct_249):S_Prim_BiasAdd[format: "NCHW", input_names: ["x", "b"], output_names: ["output"], data_format: "NCHW"](%1, %para53_L_fc3.bias)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:629/            x = self.bias_add(x, self.bias)/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/fc3-Dense)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:632/        if len(x_shape) != 2:/
}
# Order:
#   1: @L_mindspore_nn_layer_basic_Dense_construct_380:CNode_425{[0]: ValueNode<Primitive> Return, [1]: x}


subgraph attr:
subgraph instance: flatten_389 : 0000021629DF0540
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_389 parent: [subgraph @flatten_163]() {
  %1(CNode_426) = S_Prim_negative(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1741/            return reshape_(input, (-1,))/
  %2(CNode_427) = S_Prim_MakeTuple(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1741/            return reshape_(input, (-1,))/
  %3(CNode_428) = S_Prim_Reshape[input_names: ["tensor", "shape"], output_names: ["output"]](%para58_input, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1741/            return reshape_(input, (-1,))/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1741/            return reshape_(input, (-1,))/
}
# Order:
#   1: @flatten_389:CNode_426{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: ValueNode<Int64Imm> 1}
#   2: @flatten_389:CNode_427{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: CNode_426}
#   3: @flatten_389:CNode_428{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Reshape, [1]: param_input, [2]: CNode_427}
#   4: @flatten_389:CNode_429{[0]: ValueNode<Primitive> Return, [1]: CNode_428}


subgraph attr:
subgraph instance: flatten_390 : 0000021629DF7A20
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_390 parent: [subgraph @flatten_345]() {
  %1(CNode_431) = call @flatten_430()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1740/        if x_rank in (0, 1):/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1740/        if x_rank in (0, 1):/
}
# Order:
#   1: @flatten_390:CNode_431{[0]: ValueNode<FuncGraph> flatten_430}
#   2: @flatten_390:CNode_432{[0]: ValueNode<Primitive> Return, [1]: CNode_431}


subgraph attr:
after_block : 1
subgraph instance: flatten_393 : 0000021629DEEAB0
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_393 parent: [subgraph @flatten_163](%para72_) {
  %1(CNode_433) = S_Prim_equal(%para60_start_dim, I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
  %2(CNode_434) = Cond(%1, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
  %3(CNode_435) = Switch(%2, @flatten_436, @flatten_437)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
  %4(CNode_438) = %3()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
  %5(CNode_439) = Cond(%4, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
  %6(CNode_440) = Switch(%5, @flatten_441, @flatten_442)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
  %7(CNode_443) = %6()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
  Return(%7)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
}
# Order:
#   1: @flatten_393:x_shape{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Shape, [1]: param_phi_input}
#   2: @flatten_393:x_rank{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Rank, [1]: param_phi_input}
#   3: @flatten_393:CNode_433{[0]: ValueNode<DoSignaturePrimitive> S_Prim_equal, [1]: param_start_dim, [2]: ValueNode<Int64Imm> 1}
#   4: @flatten_393:CNode_434{[0]: ValueNode<Primitive> Cond, [1]: CNode_433, [2]: ValueNode<BoolImm> false}
#   5: @flatten_393:CNode_435{[0]: ValueNode<Primitive> Switch, [1]: CNode_434, [2]: ValueNode<FuncGraph> flatten_436, [3]: ValueNode<FuncGraph> flatten_437}
#   6: @flatten_393:CNode_438{[0]: CNode_435}
#   7: @flatten_393:CNode_439{[0]: ValueNode<Primitive> Cond, [1]: CNode_438, [2]: ValueNode<BoolImm> false}
#   8: @flatten_393:CNode_440{[0]: ValueNode<Primitive> Switch, [1]: CNode_439, [2]: ValueNode<FuncGraph> flatten_441, [3]: ValueNode<FuncGraph> flatten_442}
#   9: @flatten_393:CNode_443{[0]: CNode_440}
#  10: @flatten_393:CNode_444{[0]: ValueNode<Primitive> Return, [1]: CNode_443}


subgraph attr:
training : 1
subgraph instance: mindspore_nn_layer_pooling_MaxPool2d_construct_399 : 00000216313D89B0
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:568/    def construct(self, x):/
subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_399 parent: [subgraph @mindspore_nn_layer_pooling_MaxPool2d_construct_329]() {
  Return(%para68_phi_out)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/max_pool2d-MaxPool2d)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\pooling.py:589/        return out/
}
# Order:
#   1: @mindspore_nn_layer_pooling_MaxPool2d_construct_399:CNode_445{[0]: ValueNode<Primitive> Return, [1]: param_phi_out}


subgraph attr:
training : 1
subgraph instance: get_loss_412 : 0000021629DFEF00
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:125/    def get_loss(self, x, weights=1.0):/
subgraph @get_loss_412 parent: [subgraph @get_loss_303]() {
  %1(weights) = $(get_loss_280):S_Prim_Cast[input_names: ["x", "dst_type"], output_names: ["output"]](%para67_weights, F32)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:141/        weights = self.cast(weights, mstype.float32)/
  %2(x) = $(get_loss_280):S_Prim_Cast[input_names: ["x", "dst_type"], output_names: ["output"]](%para66_x, F32)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:140/        x = self.cast(x, mstype.float32)/
  %3(x) = $(get_loss_280):S_Prim_Mul[input_names: ["x", "y"], output_names: ["output"]](%1, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:142/        x = self.mul(weights, x)/
  %4(CNode_334) = $(get_loss_303):call @get_axis_335(%3)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:144/            x = self.reduce_mean(x, self.get_axis(x))/
  %5(x) = $(get_loss_303):S_Prim_ReduceMean[keep_dims: Bool(0), input_names: ["input_x", "axis"], output_names: ["y"]](%3, %4)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:144/            x = self.reduce_mean(x, self.get_axis(x))/
  %6(input_dtype) = $(get_loss_280):getattr(%para66_x, "dtype")
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:139/        input_dtype = x.dtype/
  %7(x) = S_Prim_Cast[input_names: ["x", "dst_type"], output_names: ["output"]](%5, %6)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:147/        x = self.cast(x, input_dtype)/
  Return(%7)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\loss\loss.py:148/        return x/
}
# Order:
#   1: @get_loss_412:x{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Cast, [1]: x, [2]: input_dtype}
#   2: @get_loss_412:CNode_446{[0]: ValueNode<Primitive> Return, [1]: x}


subgraph attr:
subgraph instance: L_shape_419 : 0000021629DFCF20
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1484/def shape(input_x):/
subgraph @L_shape_419(%para73_input_x) {
  %1(CNode_277) = S_Prim_Shape(%para73_input_x)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1510/    return shape_(input_x)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_loss_fn-SoftmaxCrossEntropyWithLogits)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1510/    return shape_(input_x)/
}
# Order:
#   1: @L_shape_419:CNode_277{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Shape, [1]: param_input_x}
#   2: @L_shape_419:CNode_278{[0]: ValueNode<Primitive> Return, [1]: CNode_277}


subgraph attr:
after_block : 1
subgraph instance: flatten_430 : 0000021629DF7F70
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_430 parent: [subgraph @flatten_345]() {
  %1(CNode_448) = call @_get_cache_prim_447(ClassType)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1744/        input = _get_cache_prim(P.Transpose)()(input, new_order)/
  %2(CNode_449) = %1()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1744/        input = _get_cache_prim(P.Transpose)()(input, new_order)/
  %3(x_rank) = $(flatten_345):S_Prim_Rank(%para58_input)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1738/        x_rank = rank_(input)/
  %4(perm) = S_Prim_make_range(I64(0), %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1742/        perm = ops.make_range(0, x_rank)/
  %5(new_order) = S_Prim_tuple_reversed(%4)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1743/        new_order = ops.tuple_reversed(perm)/
  %6(input) = %2(%para58_input, %5)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1744/        input = _get_cache_prim(P.Transpose)()(input, new_order)/
  %7(CNode_450) = call @flatten_393(%6)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  Return(%7)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1738/        x_rank = rank_(input)/
}
# Order:
#   1: @flatten_430:perm{[0]: ValueNode<DoSignaturePrimitive> S_Prim_make_range, [1]: ValueNode<Int64Imm> 0, [2]: x_rank}
#   2: @flatten_430:new_order{[0]: ValueNode<DoSignaturePrimitive> S_Prim_tuple_reversed, [1]: perm}
#   3: @flatten_430:CNode_448{[0]: ValueNode<FuncGraph> _get_cache_prim_447, [1]: ValueNode<ClassType> class 'mindspore.ops.operations.array_ops.Transpose'}
#   4: @flatten_430:CNode_449{[0]: CNode_448}
#   5: @flatten_430:input{[0]: CNode_449, [1]: param_input, [2]: new_order}
#   6: @flatten_430:CNode_451{[0]: ValueNode<Primitive> Return, [1]: CNode_450}
#   7: @flatten_430:CNode_450{[0]: ValueNode<FuncGraph> flatten_393, [1]: input}


subgraph attr:
subgraph instance: flatten_441 : 0000021629DF3510
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_441 parent: [subgraph @flatten_393]() {
  %1(x_rank) = $(flatten_393):S_Prim_Rank(%para72_phi_input)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1748/    x_rank = rank_(input)/
  %2(CNode_452) = S_Prim_MakeTuple(I64(0), I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1750/        if x_rank in (0, 1):/
  %3(CNode_453) = S_Prim_in(%1, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1750/        if x_rank in (0, 1):/
  %4(CNode_454) = Cond(%3, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1750/        if x_rank in (0, 1):/
  %5(CNode_455) = Switch(%4, @flatten_456, @flatten_457)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1750/        if x_rank in (0, 1):/
  %6(CNode_458) = %5()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1750/        if x_rank in (0, 1):/
  Return(%6)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1750/        if x_rank in (0, 1):/
}
# Order:
#   1: @flatten_441:CNode_452{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: ValueNode<Int64Imm> 0, [2]: ValueNode<Int64Imm> 1}
#   2: @flatten_441:CNode_453{[0]: ValueNode<DoSignaturePrimitive> S_Prim_in, [1]: x_rank, [2]: CNode_452}
#   3: @flatten_441:CNode_454{[0]: ValueNode<Primitive> Cond, [1]: CNode_453, [2]: ValueNode<BoolImm> false}
#   4: @flatten_441:CNode_455{[0]: ValueNode<Primitive> Switch, [1]: CNode_454, [2]: ValueNode<FuncGraph> flatten_456, [3]: ValueNode<FuncGraph> flatten_457}
#   5: @flatten_441:CNode_458{[0]: CNode_455}
#   6: @flatten_441:CNode_459{[0]: ValueNode<Primitive> Return, [1]: CNode_458}


subgraph attr:
subgraph instance: flatten_442 : 0000021629DE95B0
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_442 parent: [subgraph @flatten_393]() {
  %1(CNode_461) = call @flatten_460()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
}
# Order:
#   1: @flatten_442:CNode_461{[0]: ValueNode<FuncGraph> flatten_460}
#   2: @flatten_442:CNode_462{[0]: ValueNode<Primitive> Return, [1]: CNode_461}


subgraph attr:
subgraph instance: flatten_436 : 0000021629DED570
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_436 parent: [subgraph @flatten_163]() {
  %1(CNode_463) = S_Prim_negative(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
  %2(CNode_464) = S_Prim_equal(%para61_end_dim, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
}
# Order:
#   1: @flatten_436:CNode_463{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: ValueNode<Int64Imm> 1}
#   2: @flatten_436:CNode_464{[0]: ValueNode<DoSignaturePrimitive> S_Prim_equal, [1]: param_end_dim, [2]: CNode_463}
#   3: @flatten_436:CNode_465{[0]: ValueNode<Primitive> Return, [1]: CNode_464}


subgraph attr:
subgraph instance: flatten_437 : 0000021629DEA5A0
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_437 parent: [subgraph @flatten_393]() {
  %1(CNode_433) = $(flatten_393):S_Prim_equal(%para60_start_dim, I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1749/    if start_dim == 1 and end_dim == -1:/
}
# Order:
#   1: @flatten_437:CNode_466{[0]: ValueNode<Primitive> Return, [1]: CNode_433}


subgraph attr:
subgraph instance: _get_cache_prim_447 : 0000021629DF0FE0
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\_primitive_cache.py:35/def _get_cache_prim(cls: Primitive) -> Primitive:/
subgraph @_get_cache_prim_447(%para74_cls) {
  %1(CNode_468) = call @_get_cache_prim_467()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\_primitive_cache.py:88/    if _is_need_compile(_temp_func): # @jit.cond: True/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\_primitive_cache.py:88/    if _is_need_compile(_temp_func): # @jit.cond: True/
}
# Order:
#   1: @_get_cache_prim_447:CNode_468{[0]: ValueNode<FuncGraph> _get_cache_prim_467}
#   2: @_get_cache_prim_447:CNode_469{[0]: ValueNode<Primitive> Return, [1]: CNode_468}


subgraph attr:
subgraph instance: flatten_456 : 0000021629DF6A30
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_456 parent: [subgraph @flatten_393]() {
  %1(CNode_470) = S_Prim_negative(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1751/            return reshape_(input, (-1,))/
  %2(CNode_471) = S_Prim_MakeTuple(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1751/            return reshape_(input, (-1,))/
  %3(CNode_472) = S_Prim_Reshape[input_names: ["tensor", "shape"], output_names: ["output"]](%para72_phi_input, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1751/            return reshape_(input, (-1,))/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1751/            return reshape_(input, (-1,))/
}
# Order:
#   1: @flatten_456:CNode_470{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: ValueNode<Int64Imm> 1}
#   2: @flatten_456:CNode_471{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: CNode_470}
#   3: @flatten_456:CNode_472{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Reshape, [1]: param_phi_input, [2]: CNode_471}
#   4: @flatten_456:CNode_473{[0]: ValueNode<Primitive> Return, [1]: CNode_472}


subgraph attr:
subgraph instance: flatten_457 : 0000021629DF1A80
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_457 parent: [subgraph @flatten_393]() {
  %1(CNode_475) = call @flatten_474()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1750/        if x_rank in (0, 1):/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1750/        if x_rank in (0, 1):/
}
# Order:
#   1: @flatten_457:CNode_475{[0]: ValueNode<FuncGraph> flatten_474}
#   2: @flatten_457:CNode_476{[0]: ValueNode<Primitive> Return, [1]: CNode_475}


subgraph attr:
after_block : 1
subgraph instance: flatten_460 : 0000021629DEE010
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_460 parent: [subgraph @flatten_393]() {
  %1(x_rank) = $(flatten_393):S_Prim_Rank(%para72_phi_input)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1748/    x_rank = rank_(input)/
  %2(idx) = call @canonicalize_axis_477(%para60_start_dim, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1755/    start_dim = canonicalize_axis(start_dim, x_rank)/
  %3(end_dim) = call @canonicalize_axis_477(%para61_end_dim, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1756/    end_dim = canonicalize_axis(end_dim, x_rank)/
  %4(CNode_479) = call @check_dim_valid_478(%2, %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1757/    check_dim_valid(start_dim, end_dim)/
  %5(CNode_480) = StopGradient(%4)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
  %6(CNode_481) = S_Prim_MakeTuple(I64(0), I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1759/    if x_rank in (0, 1):/
  %7(CNode_482) = S_Prim_in(%1, %6)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1759/    if x_rank in (0, 1):/
  %8(CNode_483) = Cond(%7, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1759/    if x_rank in (0, 1):/
  %9(CNode_484) = Switch(%8, @flatten_485, @flatten_486)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1759/    if x_rank in (0, 1):/
  %10(CNode_487) = %9()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1759/    if x_rank in (0, 1):/
  %11(CNode_488) = Depend[side_effect_propagate: I64(1)](%10, %5)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1759/    if x_rank in (0, 1):/
  Return(%11)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1759/    if x_rank in (0, 1):/
}
# Order:
#   1: @flatten_460:idx{[0]: ValueNode<FuncGraph> canonicalize_axis_477, [1]: param_start_dim, [2]: x_rank}
#   2: @flatten_460:end_dim{[0]: ValueNode<FuncGraph> canonicalize_axis_477, [1]: param_end_dim, [2]: x_rank}
#   3: @flatten_460:CNode_479{[0]: ValueNode<FuncGraph> check_dim_valid_478, [1]: idx, [2]: end_dim}
#   4: @flatten_460:CNode_481{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: ValueNode<Int64Imm> 0, [2]: ValueNode<Int64Imm> 1}
#   5: @flatten_460:CNode_482{[0]: ValueNode<DoSignaturePrimitive> S_Prim_in, [1]: x_rank, [2]: CNode_481}
#   6: @flatten_460:CNode_483{[0]: ValueNode<Primitive> Cond, [1]: CNode_482, [2]: ValueNode<BoolImm> false}
#   7: @flatten_460:CNode_484{[0]: ValueNode<Primitive> Switch, [1]: CNode_483, [2]: ValueNode<FuncGraph> flatten_485, [3]: ValueNode<FuncGraph> flatten_486}
#   8: @flatten_460:CNode_487{[0]: CNode_484}
#   9: @flatten_460:CNode_489{[0]: ValueNode<Primitive> Return, [1]: CNode_488}


subgraph attr:
subgraph instance: _get_cache_prim_467 : 0000021629DF74D0
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\_primitive_cache.py:35/def _get_cache_prim(cls: Primitive) -> Primitive:/
subgraph @_get_cache_prim_467 parent: [subgraph @_get_cache_prim_447]() {
  Return(@_new_prim_for_graph_490)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\_primitive_cache.py:89/        return _new_prim_for_graph/
}
# Order:
#   1: @_get_cache_prim_467:CNode_491{[0]: ValueNode<Primitive> Return, [1]: ValueNode<FuncGraph> _new_prim_for_graph_490}


subgraph attr:
after_block : 1
subgraph instance: flatten_474 : 0000021629DF54F0
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_474 parent: [subgraph @flatten_393]() {
  %1(CNode_492) = call @_get_cache_prim_447(ClassType)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1752/        return _get_cache_prim(P.Flatten)()(input)/
  %2(CNode_493) = %1()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1752/        return _get_cache_prim(P.Flatten)()(input)/
  %3(CNode_494) = %2(%para72_phi_input)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1752/        return _get_cache_prim(P.Flatten)()(input)/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1752/        return _get_cache_prim(P.Flatten)()(input)/
}
# Order:
#   1: @flatten_474:CNode_492{[0]: ValueNode<FuncGraph> _get_cache_prim_447, [1]: ValueNode<ClassType> class 'mindspore.ops.operations.nn_ops.Flatten'}
#   2: @flatten_474:CNode_493{[0]: CNode_492}
#   3: @flatten_474:CNode_494{[0]: CNode_493, [1]: param_phi_input}
#   4: @flatten_474:CNode_495{[0]: ValueNode<Primitive> Return, [1]: CNode_494}


subgraph attr:
subgraph instance: check_dim_valid_478 : 0000021629DF4FA0
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1721/    def check_dim_valid(start_dim, end_dim):/
subgraph @check_dim_valid_478(%para75_start_dim, %para76_end_dim) {
  %1(CNode_496) = S_Prim_greater(%para75_start_dim, %para76_end_dim)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1722/        if start_dim > end_dim:/
  %2(CNode_497) = Cond(%1, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1722/        if start_dim > end_dim:/
  %3(CNode_498) = Switch(%2, @check_dim_valid_499, @check_dim_valid_500)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1722/        if start_dim > end_dim:/
  %4(CNode_501) = %3()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1722/        if start_dim > end_dim:/
  Return(%4)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1722/        if start_dim > end_dim:/
}
# Order:
#   1: @check_dim_valid_478:CNode_496{[0]: ValueNode<DoSignaturePrimitive> S_Prim_greater, [1]: param_start_dim, [2]: param_end_dim}
#   2: @check_dim_valid_478:CNode_497{[0]: ValueNode<Primitive> Cond, [1]: CNode_496, [2]: ValueNode<BoolImm> false}
#   3: @check_dim_valid_478:CNode_498{[0]: ValueNode<Primitive> Switch, [1]: CNode_497, [2]: ValueNode<FuncGraph> check_dim_valid_499, [3]: ValueNode<FuncGraph> check_dim_valid_500}
#   4: @check_dim_valid_478:CNode_501{[0]: CNode_498}
#   5: @check_dim_valid_478:CNode_502{[0]: ValueNode<Primitive> Return, [1]: CNode_501}


subgraph attr:
subgraph instance: canonicalize_axis_477 : 0000021629DED020
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1725/    def canonicalize_axis(axis, x_rank):/
subgraph @canonicalize_axis_477(%para77_axis, %para78_x_rank) {
  %1(CNode_503) = S_Prim_not_equal(%para78_x_rank, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
  %2(CNode_504) = Cond(%1, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
  %3(CNode_505) = Switch(%2, @canonicalize_axis_506, @canonicalize_axis_507)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
  %4(ndim) = %3()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
  %5(CNode_509) = call @check_axis_valid_508(%para77_axis, %4)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1727/        check_axis_valid(axis, ndim)/
  %6(CNode_510) = StopGradient(%5)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1725/    def canonicalize_axis(axis, x_rank):/
  %7(CNode_511) = S_Prim_greater_equal(%para77_axis, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1728/        return axis if axis >= 0 else axis + ndim/
  %8(CNode_512) = Cond(%7, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1728/        return axis if axis >= 0 else axis + ndim/
  %9(CNode_513) = Switch(%8, @canonicalize_axis_514, @canonicalize_axis_515)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1728/        return axis if axis >= 0 else axis + ndim/
  %10(CNode_516) = %9()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1728/        return axis if axis >= 0 else axis + ndim/
  %11(CNode_517) = Depend[side_effect_propagate: I64(1)](%10, %6)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1728/        return axis if axis >= 0 else axis + ndim/
  Return(%11)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1728/        return axis if axis >= 0 else axis + ndim/
}
# Order:
#   1: @canonicalize_axis_477:CNode_503{[0]: ValueNode<DoSignaturePrimitive> S_Prim_not_equal, [1]: param_x_rank, [2]: ValueNode<Int64Imm> 0}
#   2: @canonicalize_axis_477:CNode_504{[0]: ValueNode<Primitive> Cond, [1]: CNode_503, [2]: ValueNode<BoolImm> false}
#   3: @canonicalize_axis_477:CNode_505{[0]: ValueNode<Primitive> Switch, [1]: CNode_504, [2]: ValueNode<FuncGraph> canonicalize_axis_506, [3]: ValueNode<FuncGraph> canonicalize_axis_507}
#   4: @canonicalize_axis_477:ndim{[0]: CNode_505}
#   5: @canonicalize_axis_477:CNode_509{[0]: ValueNode<FuncGraph> check_axis_valid_508, [1]: param_axis, [2]: ndim}
#   6: @canonicalize_axis_477:CNode_511{[0]: ValueNode<DoSignaturePrimitive> S_Prim_greater_equal, [1]: param_axis, [2]: ValueNode<Int64Imm> 0}
#   7: @canonicalize_axis_477:CNode_512{[0]: ValueNode<Primitive> Cond, [1]: CNode_511, [2]: ValueNode<BoolImm> false}
#   8: @canonicalize_axis_477:CNode_513{[0]: ValueNode<Primitive> Switch, [1]: CNode_512, [2]: ValueNode<FuncGraph> canonicalize_axis_514, [3]: ValueNode<FuncGraph> canonicalize_axis_515}
#   9: @canonicalize_axis_477:CNode_516{[0]: CNode_513}
#  10: @canonicalize_axis_477:CNode_518{[0]: ValueNode<Primitive> Return, [1]: CNode_517}


subgraph attr:
subgraph instance: flatten_485 : 0000021629DE9B00
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_485 parent: [subgraph @flatten_393]() {
  %1(CNode_519) = S_Prim_negative(I64(1))
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1760/        return reshape_(input, (-1,))/
  %2(CNode_520) = S_Prim_MakeTuple(%1)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1760/        return reshape_(input, (-1,))/
  %3(CNode_521) = S_Prim_Reshape[input_names: ["tensor", "shape"], output_names: ["output"]](%para72_phi_input, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1760/        return reshape_(input, (-1,))/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1760/        return reshape_(input, (-1,))/
}
# Order:
#   1: @flatten_485:CNode_519{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: ValueNode<Int64Imm> 1}
#   2: @flatten_485:CNode_520{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: CNode_519}
#   3: @flatten_485:CNode_521{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Reshape, [1]: param_phi_input, [2]: CNode_520}
#   4: @flatten_485:CNode_522{[0]: ValueNode<Primitive> Return, [1]: CNode_521}


subgraph attr:
subgraph instance: flatten_486 : 0000021629DEFAA0
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_486 parent: [subgraph @flatten_460]() {
  %1(CNode_524) = call @flatten_523()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1759/    if x_rank in (0, 1):/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1759/    if x_rank in (0, 1):/
}
# Order:
#   1: @flatten_486:CNode_524{[0]: ValueNode<FuncGraph> flatten_523}
#   2: @flatten_486:CNode_525{[0]: ValueNode<Primitive> Return, [1]: CNode_524}


subgraph attr:
subgraph instance: _new_prim_for_graph_490 : 0000021629DF5A40
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\_primitive_cache.py:67/    def _new_prim_for_graph(*args, **kwargs) -> Primitive:/
subgraph @_new_prim_for_graph_490 parent: [subgraph @_get_cache_prim_447](%para79_args, %para80_kwargs) {
  %1(CNode_526) = UnpackCall_unpack_call(%para74_cls, %para79_args, %para80_kwargs)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\_primitive_cache.py:68/        return cls(*args, **kwargs)/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\_primitive_cache.py:68/        return cls(*args, **kwargs)/
}
# Order:
#   1: @_new_prim_for_graph_490:CNode_526{[0]: ValueNode<UnpackCall> MetaFuncGraph-unpack_call.527, [1]: param_cls, [2]: param_args, [3]: param_kwargs}
#   2: @_new_prim_for_graph_490:CNode_528{[0]: ValueNode<Primitive> Return, [1]: CNode_526}


subgraph attr:
subgraph instance: check_dim_valid_499 : 0000021629DF6F80
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1721/    def check_dim_valid(start_dim, end_dim):/
subgraph @check_dim_valid_499() {
  %1(CNode_529) = raise[side_effect_io: Bool(1)]("ValueError", "For 'flatten', 'start_dim' cannot come after 'end_dim'.", "None")
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1723/            raise ValueError("For 'flatten', 'start_dim' cannot come after 'end_dim'.")/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1723/            raise ValueError("For 'flatten', 'start_dim' cannot come after 'end_dim'.")/
}
# Order:
#   1: @check_dim_valid_499:CNode_529{[0]: ValueNode<Primitive> raise, [1]: ValueNode<StringImm> ValueError, [2]: ValueNode<StringImm> For 'flatten', 'start_dim' cannot come after 'end_dim'., [3]: ValueNode<StringImm> None}
#   2: @check_dim_valid_499:CNode_530{[0]: ValueNode<Primitive> Return, [1]: CNode_529}


subgraph attr:
subgraph instance: check_dim_valid_500 : 0000021629DF0A90
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1721/    def check_dim_valid(start_dim, end_dim):/
subgraph @check_dim_valid_500() {
  %1(CNode_532) = call @check_dim_valid_531()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1722/        if start_dim > end_dim:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1722/        if start_dim > end_dim:/
}
# Order:
#   1: @check_dim_valid_500:CNode_532{[0]: ValueNode<FuncGraph> check_dim_valid_531}
#   2: @check_dim_valid_500:CNode_533{[0]: ValueNode<Primitive> Return, [1]: CNode_532}


subgraph attr:
subgraph instance: check_axis_valid_508 : 0000021629DF4500
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1717/    def check_axis_valid(axis, ndim):/
subgraph @check_axis_valid_508(%para81_axis, %para82_ndim) {
  %1(CNode_534) = S_Prim_negative(%para82_ndim)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  %2(CNode_535) = S_Prim_less(%para81_axis, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  %3(CNode_536) = Cond(%2, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  %4(CNode_537) = Switch(%3, @check_axis_valid_538, @check_axis_valid_539)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  %5(CNode_540) = %4()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  %6(CNode_541) = Cond(%5, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  %7(CNode_542) = Switch(%6, @check_axis_valid_543, @check_axis_valid_544)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  %8(CNode_545) = %7()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  Return(%8)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1718/        if axis < -ndim or axis >= ndim:/
}
# Order:
#   1: @check_axis_valid_508:CNode_534{[0]: ValueNode<DoSignaturePrimitive> S_Prim_negative, [1]: param_ndim}
#   2: @check_axis_valid_508:CNode_535{[0]: ValueNode<DoSignaturePrimitive> S_Prim_less, [1]: param_axis, [2]: CNode_534}
#   3: @check_axis_valid_508:CNode_536{[0]: ValueNode<Primitive> Cond, [1]: CNode_535, [2]: ValueNode<BoolImm> false}
#   4: @check_axis_valid_508:CNode_537{[0]: ValueNode<Primitive> Switch, [1]: CNode_536, [2]: ValueNode<FuncGraph> check_axis_valid_538, [3]: ValueNode<FuncGraph> check_axis_valid_539}
#   5: @check_axis_valid_508:CNode_540{[0]: CNode_537}
#   6: @check_axis_valid_508:CNode_541{[0]: ValueNode<Primitive> Cond, [1]: CNode_540, [2]: ValueNode<BoolImm> false}
#   7: @check_axis_valid_508:CNode_542{[0]: ValueNode<Primitive> Switch, [1]: CNode_541, [2]: ValueNode<FuncGraph> check_axis_valid_543, [3]: ValueNode<FuncGraph> check_axis_valid_544}
#   8: @check_axis_valid_508:CNode_545{[0]: CNode_542}
#   9: @check_axis_valid_508:CNode_546{[0]: ValueNode<Primitive> Return, [1]: CNode_545}


subgraph attr:
subgraph instance: canonicalize_axis_506 : 0000021629DF3A60
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
subgraph @canonicalize_axis_506 parent: [subgraph @canonicalize_axis_477]() {
  Return(%para78_x_rank)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
}
# Order:
#   1: @canonicalize_axis_506:CNode_547{[0]: ValueNode<Primitive> Return, [1]: param_x_rank}


subgraph attr:
subgraph instance: canonicalize_axis_507 : 0000021629DEFFF0
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
subgraph @canonicalize_axis_507() {
  Return(I64(1))
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
}
# Order:
#   1: @canonicalize_axis_507:CNode_548{[0]: ValueNode<Primitive> Return, [1]: ValueNode<Int64Imm> 1}


subgraph attr:
subgraph instance: canonicalize_axis_514 : 0000021629DE8070
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1728/        return axis if axis >= 0 else axis + ndim/
subgraph @canonicalize_axis_514 parent: [subgraph @canonicalize_axis_477]() {
  Return(%para77_axis)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1728/        return axis if axis >= 0 else axis + ndim/
}
# Order:
#   1: @canonicalize_axis_514:CNode_549{[0]: ValueNode<Primitive> Return, [1]: param_axis}


subgraph attr:
subgraph instance: canonicalize_axis_515 : 0000021629DE85C0
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1728/        return axis if axis >= 0 else axis + ndim/
subgraph @canonicalize_axis_515 parent: [subgraph @canonicalize_axis_477]() {
  %1(CNode_503) = $(canonicalize_axis_477):S_Prim_not_equal(%para78_x_rank, I64(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
  %2(CNode_504) = $(canonicalize_axis_477):Cond(%1, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
  %3(CNode_505) = $(canonicalize_axis_477):Switch(%2, @canonicalize_axis_506, @canonicalize_axis_507)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
  %4(ndim) = $(canonicalize_axis_477):%3()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1726/        ndim = x_rank if x_rank != 0 else 1/
  %5(CNode_550) = S_Prim_add(%para77_axis, %4)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1728/        return axis if axis >= 0 else axis + ndim/
  Return(%5)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1728/        return axis if axis >= 0 else axis + ndim/
}
# Order:
#   1: @canonicalize_axis_515:CNode_550{[0]: ValueNode<DoSignaturePrimitive> S_Prim_add, [1]: param_axis, [2]: ndim}
#   2: @canonicalize_axis_515:CNode_551{[0]: ValueNode<Primitive> Return, [1]: CNode_550}


subgraph attr:
after_block : 1
subgraph instance: flatten_523 : 0000021629DEDAC0
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_523 parent: [subgraph @flatten_460]() {
  %1(x_rank) = $(flatten_393):S_Prim_Rank(%para72_phi_input)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1748/    x_rank = rank_(input)/
  %2(idx) = $(flatten_460):call @canonicalize_axis_477(%para60_start_dim, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1755/    start_dim = canonicalize_axis(start_dim, x_rank)/
  %3(end_dim) = $(flatten_460):call @canonicalize_axis_477(%para61_end_dim, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1756/    end_dim = canonicalize_axis(end_dim, x_rank)/
  %4(CNode_552) = S_Prim_equal(%2, %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1762/    if start_dim == end_dim:/
  %5(CNode_553) = Cond(%4, Bool(0))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1762/    if start_dim == end_dim:/
  %6(CNode_554) = Switch(%5, @flatten_555, @flatten_556)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1762/    if start_dim == end_dim:/
  %7(CNode_557) = %6()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1762/    if start_dim == end_dim:/
  Return(%7)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1762/    if start_dim == end_dim:/
}
# Order:
#   1: @flatten_523:CNode_552{[0]: ValueNode<DoSignaturePrimitive> S_Prim_equal, [1]: idx, [2]: end_dim}
#   2: @flatten_523:CNode_553{[0]: ValueNode<Primitive> Cond, [1]: CNode_552, [2]: ValueNode<BoolImm> false}
#   3: @flatten_523:CNode_554{[0]: ValueNode<Primitive> Switch, [1]: CNode_553, [2]: ValueNode<FuncGraph> flatten_555, [3]: ValueNode<FuncGraph> flatten_556}
#   4: @flatten_523:CNode_557{[0]: CNode_554}
#   5: @flatten_523:CNode_558{[0]: ValueNode<Primitive> Return, [1]: CNode_557}


subgraph attr:
after_block : 1
subgraph instance: check_dim_valid_531 : 0000021629DF2FC0
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1721/    def check_dim_valid(start_dim, end_dim):/
subgraph @check_dim_valid_531() {
  Return(None)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
}
# Order:
#   1: @check_dim_valid_531:CNode_559{[0]: ValueNode<Primitive> Return, [1]: ValueNode<None> None}


subgraph attr:
subgraph instance: check_axis_valid_543 : 0000021629DF2A70
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1717/    def check_axis_valid(axis, ndim):/
subgraph @check_axis_valid_543() {
  %1(CNode_560) = raise[side_effect_io: Bool(1)]("ValueError", "'start_dim' or 'end_dim' out of range.", "None")
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1719/            raise ValueError("'start_dim' or 'end_dim' out of range.")/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1719/            raise ValueError("'start_dim' or 'end_dim' out of range.")/
}
# Order:
#   1: @check_axis_valid_543:CNode_560{[0]: ValueNode<Primitive> raise, [1]: ValueNode<StringImm> ValueError, [2]: ValueNode<StringImm> 'start_dim' or 'end_dim' out of range., [3]: ValueNode<StringImm> None}
#   2: @check_axis_valid_543:CNode_561{[0]: ValueNode<Primitive> Return, [1]: CNode_560}


subgraph attr:
subgraph instance: check_axis_valid_544 : 0000021629DF2520
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1717/    def check_axis_valid(axis, ndim):/
subgraph @check_axis_valid_544() {
  %1(CNode_563) = call @check_axis_valid_562()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1718/        if axis < -ndim or axis >= ndim:/
}
# Order:
#   1: @check_axis_valid_544:CNode_563{[0]: ValueNode<FuncGraph> check_axis_valid_562}
#   2: @check_axis_valid_544:CNode_564{[0]: ValueNode<Primitive> Return, [1]: CNode_563}


subgraph attr:
subgraph instance: check_axis_valid_538 : 0000021629DF3FB0
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1717/    def check_axis_valid(axis, ndim):/
subgraph @check_axis_valid_538 parent: [subgraph @check_axis_valid_508]() {
  %1(CNode_534) = $(check_axis_valid_508):S_Prim_negative(%para82_ndim)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  %2(CNode_535) = $(check_axis_valid_508):S_Prim_less(%para81_axis, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  Return(%2)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1718/        if axis < -ndim or axis >= ndim:/
}
# Order:
#   1: @check_axis_valid_538:CNode_565{[0]: ValueNode<Primitive> Return, [1]: CNode_535}


subgraph attr:
subgraph instance: check_axis_valid_539 : 0000021629DF4A50
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1717/    def check_axis_valid(axis, ndim):/
subgraph @check_axis_valid_539 parent: [subgraph @check_axis_valid_508]() {
  %1(CNode_566) = S_Prim_greater_equal(%para81_axis, %para82_ndim)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1718/        if axis < -ndim or axis >= ndim:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1718/        if axis < -ndim or axis >= ndim:/
}
# Order:
#   1: @check_axis_valid_539:CNode_566{[0]: ValueNode<DoSignaturePrimitive> S_Prim_greater_equal, [1]: param_axis, [2]: param_ndim}
#   2: @check_axis_valid_539:CNode_567{[0]: ValueNode<Primitive> Return, [1]: CNode_566}


subgraph attr:
subgraph instance: flatten_555 : 0000021629DEBAE0
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_555 parent: [subgraph @flatten_393]() {
  Return(%para72_phi_input)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1763/        return input/
}
# Order:
#   1: @flatten_555:CNode_568{[0]: ValueNode<Primitive> Return, [1]: param_phi_input}


subgraph attr:
subgraph instance: flatten_556 : 0000021629DEE560
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_556 parent: [subgraph @flatten_460]() {
  %1(CNode_570) = call @flatten_569()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1762/    if start_dim == end_dim:/
  Return(%1)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1762/    if start_dim == end_dim:/
}
# Order:
#   1: @flatten_556:CNode_570{[0]: ValueNode<FuncGraph> flatten_569}
#   2: @flatten_556:CNode_571{[0]: ValueNode<Primitive> Return, [1]: CNode_570}


subgraph attr:
after_block : 1
subgraph instance: check_axis_valid_562 : 0000021629DF64E0
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1717/    def check_axis_valid(axis, ndim):/
subgraph @check_axis_valid_562() {
  Return(None)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
}
# Order:
#   1: @check_axis_valid_562:CNode_572{[0]: ValueNode<Primitive> Return, [1]: ValueNode<None> None}


subgraph attr:
after_block : 1
subgraph instance: flatten_569 : 0000021629DEAAF0
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_569 parent: [subgraph @flatten_460]() {
  %1(x_rank) = $(flatten_393):S_Prim_Rank(%para72_phi_input)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1748/    x_rank = rank_(input)/
  %2(idx) = $(flatten_460):call @canonicalize_axis_477(%para60_start_dim, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1755/    start_dim = canonicalize_axis(start_dim, x_rank)/
  %3(CNode_574) = call @flatten_573(%2, I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  Return(%3)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1767/    while idx <= end_dim:/
}
# Order:
#   1: @flatten_569:CNode_575{[0]: ValueNode<Primitive> Return, [1]: CNode_574}
#   2: @flatten_569:CNode_574{[0]: ValueNode<FuncGraph> flatten_573, [1]: idx, [2]: ValueNode<Int64Imm> 1}


subgraph attr:
is_while_header : 1
subgraph instance: flatten_573 : 0000021629DE8B10
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_573 parent: [subgraph @flatten_460](%para83_, %para84_) {
  %1(x_rank) = $(flatten_393):S_Prim_Rank(%para72_phi_input)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1748/    x_rank = rank_(input)/
  %2(end_dim) = $(flatten_460):call @canonicalize_axis_477(%para61_end_dim, %1)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1756/    end_dim = canonicalize_axis(end_dim, x_rank)/
  %3(CNode_576) = S_Prim_less_equal(%para83_phi_idx, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1767/    while idx <= end_dim:/
  %4(force_while_cond_CNode_576) = Cond(%3, Bool(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1767/    while idx <= end_dim:/
  %5(CNode_577) = Switch(%4, @flatten_578, @flatten_579)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1767/    while idx <= end_dim:/
  %6(CNode_580) = %5()
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1767/    while idx <= end_dim:/
  Return(%6)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1767/    while idx <= end_dim:/
}
# Order:
#   1: @flatten_573:CNode_576{[0]: ValueNode<DoSignaturePrimitive> S_Prim_less_equal, [1]: param_phi_idx, [2]: end_dim}
#   2: @flatten_573:force_while_cond_CNode_576{[0]: ValueNode<Primitive> Cond, [1]: CNode_576, [2]: ValueNode<BoolImm> true}
#   3: @flatten_573:CNode_577{[0]: ValueNode<Primitive> Switch, [1]: force_while_cond_CNode_576, [2]: ValueNode<FuncGraph> flatten_578, [3]: ValueNode<FuncGraph> flatten_579}
#   4: @flatten_573:CNode_580{[0]: CNode_577}
#   5: @flatten_573:CNode_581{[0]: ValueNode<Primitive> Return, [1]: CNode_580}


subgraph attr:
subgraph instance: flatten_578 : 0000021629DEF550
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_578 parent: [subgraph @flatten_573]() {
  %1(idx) = S_Prim_add(%para83_phi_idx, I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1769/        idx += 1/
  %2(x_shape) = $(flatten_393):S_Prim_Shape(%para72_phi_input)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1747/    x_shape = shape_(input)/
  %3(CNode_582) = S_Prim_getitem(%2, %para83_phi_idx)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1768/        dim_length *= x_shape[idx]/
  %4(dim_length) = S_Prim_mul(%para84_phi_dim_length, %3)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1768/        dim_length *= x_shape[idx]/
  %5(CNode_583) = call @flatten_573(%1, %4)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\nn\layer\basic.py:466/        return F.flatten(x, start_dim=self.start_dim, end_dim=self.end_dim)/
  Return(%5)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1767/    while idx <= end_dim:/
}
# Order:
#   1: @flatten_578:CNode_582{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: x_shape, [2]: param_phi_idx}
#   2: @flatten_578:dim_length{[0]: ValueNode<DoSignaturePrimitive> S_Prim_mul, [1]: param_phi_dim_length, [2]: CNode_582}
#   3: @flatten_578:idx{[0]: ValueNode<DoSignaturePrimitive> S_Prim_add, [1]: param_phi_idx, [2]: ValueNode<Int64Imm> 1}
#   4: @flatten_578:CNode_584{[0]: ValueNode<Primitive> Return, [1]: CNode_583}
#   5: @flatten_578:CNode_583{[0]: ValueNode<FuncGraph> flatten_573, [1]: idx, [2]: dim_length}


subgraph attr:
subgraph instance: flatten_579 : 0000021629DEB040
# In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1678/def flatten(input, order='C', *, start_dim=1, end_dim=-1):/
subgraph @flatten_579 parent: [subgraph @flatten_573]() {
  %1(x_shape) = $(flatten_393):S_Prim_Shape(%para72_phi_input)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1747/    x_shape = shape_(input)/
  %2(x_rank) = $(flatten_393):S_Prim_Rank(%para72_phi_input)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1748/    x_rank = rank_(input)/
  %3(idx) = $(flatten_460):call @canonicalize_axis_477(%para60_start_dim, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1755/    start_dim = canonicalize_axis(start_dim, x_rank)/
  %4(CNode_585) = S_Prim_make_slice(None, %3, None)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1770/    new_shape = x_shape[:start_dim] + (dim_length,) + x_shape[end_dim + 1:]/
  %5(CNode_586) = S_Prim_getitem(%1, %4)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1770/    new_shape = x_shape[:start_dim] + (dim_length,) + x_shape[end_dim + 1:]/
  %6(CNode_587) = S_Prim_MakeTuple(%para84_phi_dim_length)
      : (<null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1770/    new_shape = x_shape[:start_dim] + (dim_length,) + x_shape[end_dim + 1:]/
  %7(CNode_588) = S_Prim_add(%5, %6)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1770/    new_shape = x_shape[:start_dim] + (dim_length,) + x_shape[end_dim + 1:]/
  %8(end_dim) = $(flatten_460):call @canonicalize_axis_477(%para61_end_dim, %2)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1756/    end_dim = canonicalize_axis(end_dim, x_rank)/
  %9(CNode_589) = S_Prim_add(%8, I64(1))
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1770/    new_shape = x_shape[:start_dim] + (dim_length,) + x_shape[end_dim + 1:]/
  %10(CNode_590) = S_Prim_make_slice(%9, None, None)
      : (<null>, <null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1770/    new_shape = x_shape[:start_dim] + (dim_length,) + x_shape[end_dim + 1:]/
  %11(CNode_591) = S_Prim_getitem(%1, %10)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1770/    new_shape = x_shape[:start_dim] + (dim_length,) + x_shape[end_dim + 1:]/
  %12(new_shape) = S_Prim_add(%7, %11)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1770/    new_shape = x_shape[:start_dim] + (dim_length,) + x_shape[end_dim + 1:]/
  %13(CNode_592) = S_Prim_Reshape[input_names: ["tensor", "shape"], output_names: ["output"]](%para72_phi_input, %12)
      : (<null>, <null>) -> (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1771/    return reshape_(input, new_shape)/
  Return(%13)
      : (<null>)
      #scope: (Default/network-WithLossCell/_backbone-LeNet5/flatten-Flatten)
      # In file D:\Anaconda\envs\mindspore\lib\site-packages\mindspore\ops\function\array_func.py:1771/    return reshape_(input, new_shape)/
}
# Order:
#   1: @flatten_579:CNode_585{[0]: ValueNode<DoSignaturePrimitive> S_Prim_make_slice, [1]: ValueNode<None> None, [2]: idx, [3]: ValueNode<None> None}
#   2: @flatten_579:CNode_586{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: x_shape, [2]: CNode_585}
#   3: @flatten_579:CNode_587{[0]: ValueNode<DoSignaturePrimitive> S_Prim_MakeTuple, [1]: param_phi_dim_length}
#   4: @flatten_579:CNode_588{[0]: ValueNode<DoSignaturePrimitive> S_Prim_add, [1]: CNode_586, [2]: CNode_587}
#   5: @flatten_579:CNode_589{[0]: ValueNode<DoSignaturePrimitive> S_Prim_add, [1]: end_dim, [2]: ValueNode<Int64Imm> 1}
#   6: @flatten_579:CNode_590{[0]: ValueNode<DoSignaturePrimitive> S_Prim_make_slice, [1]: CNode_589, [2]: ValueNode<None> None, [3]: ValueNode<None> None}
#   7: @flatten_579:CNode_591{[0]: ValueNode<DoSignaturePrimitive> S_Prim_getitem, [1]: x_shape, [2]: CNode_590}
#   8: @flatten_579:new_shape{[0]: ValueNode<DoSignaturePrimitive> S_Prim_add, [1]: CNode_588, [2]: CNode_591}
#   9: @flatten_579:CNode_592{[0]: ValueNode<DoSignaturePrimitive> S_Prim_Reshape, [1]: param_phi_input, [2]: new_shape}
#  10: @flatten_579:CNode_593{[0]: ValueNode<Primitive> Return, [1]: CNode_592}


